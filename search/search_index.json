{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-j","title":"KIT-J","text":"<p>Daniel Djanikian</p> <p>Vitor Padovani</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2 - Data 13/03/2025</li> <li> Roteiro 3 - Data 13/05/2025</li> <li> Roteiro 4 - Data 19/05/2025</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto Computa\u00e7\u00e3o em Nuvem 2025.1","text":""},{"location":"projeto/main/#etapa-1","title":"Etapa 1","text":""},{"location":"projeto/main/#11-descricao-do-projeto","title":"1.1 - Descri\u00e7\u00e3o do projeto","text":"<p>Este projeto consiste em uma API RESTful desenvolvida com FastAPI que realiza scraping de dados de criptomoedas, de uma fonte externa, e permite o acesso a essas informa\u00e7\u00f5es por meio de endpoints documentados. A aplica\u00e7\u00e3o \u00e9 containerizada com Docker, permitindo f\u00e1cil execu\u00e7\u00e3o e portabilidade.</p> <p>O foco principal \u00e9 viabilizar e verificar se ser\u00e1 poss\u00edvel fazer a coleta e disponibiliza\u00e7\u00e3o de dados atrav\u00e9s de uma API que cadastra dados de usu\u00e1rios, com deploy simples e documenta\u00e7\u00e3o acess\u00edvel.</p>"},{"location":"projeto/main/#integrantes-do-grupo","title":"Integrantes do grupo","text":"<ul> <li>Nome: Daniel Djanikian</li> <li>Nome: Vitor Padovani</li> </ul>"},{"location":"projeto/main/#repositorio","title":"Repositorio","text":"<ul> <li>Link: https://github.com/vitorpadovani/cloud_projeto</li> </ul>"},{"location":"projeto/main/#12-scrap-realizado","title":"1.2 - Scrap realizado","text":"<p>O scraping foi realizado na p\u00e1gina CoinMarketCap, que fornece informa\u00e7\u00f5es detalhadas sobre criptomoedas, incluindo pre\u00e7o, volume de negocia\u00e7\u00e3o, capitaliza\u00e7\u00e3o de mercado e outras m\u00e9tricas relevantes. A coleta de dados foi feita utilizando a biblioteca BeautifulSoup para extrair as informa\u00e7\u00f5es desejadas da p\u00e1gina HTML. Al\u00e9m disso, foram utilizados os m\u00e9todos <code>requests.Session()</code> para realizar as requisi\u00e7\u00f5es HTTP e <code>BeautifulSoup</code> para parsear o conte\u00fado HTML obtido.</p> <p>O scraping \u00e9 realizado em um intervalo de alguns segundos entre as requisi\u00e7\u00f5es, para evitar sobrecarga no servidor e respeitar as pol\u00edticas de uso do site. Os dados coletados incluem o nome da criptomoeda e seu pre\u00e7o atual de mercado, sendo que no site, o pre\u00e7o \u00e9 atualizado em tempo real, possibilitando a coleta de dados atualizados.</p> <p>Para que seja poss\u00edvel realizar o scraping, \u00e9 necess\u00e1rio que o usu\u00e1rio esteja cadastrado no sistema, o que \u00e9 feito atrav\u00e9s de um endpoint espec\u00edfico. Ap\u00f3s o cadastro, o usu\u00e1rio pode acessar os dados coletados por meio de outro endpoint, que retorna as informa\u00e7\u00f5es em formato JSON.</p>"},{"location":"projeto/main/#13-api-restful","title":"1.3 - API RESTful","text":"<p>A API RESTful foi desenvolvida utilizando o framework FastAPI, que permite a cria\u00e7\u00e3o de APIs de forma r\u00e1pida e eficiente. A API possui os seguintes endpoints:</p> <ul> <li><code>POST /registrar</code>: Cadastra um novo usu\u00e1rio no sistema.</li> <li><code>POST /login</code>: Realiza o login de um usu\u00e1rio j\u00e1 cadastrado.</li> <li><code>GET /consultar</code>: Retorna a lista das 10 criptomoedas mais valiosas, com nome e pre\u00e7o atual (em dol\u00e1res) caso o usu\u00e1rio esteja autenticado.</li> </ul> <p>Para cada endpoint, foram implementadas as seguintes funcionalidades:</p> <ul> <li>Registrar: O usu\u00e1rio deve fornecer um nome, email e senha. Todos s\u00e3o registrados na base de dados e retornar\u00e1 um token de autentica\u00e7\u00e3o JWT, em caso o cadastro seja realizado com sucesso.</li> <li>Login: O usu\u00e1rio deve fornecer email e senha. Caso as credenciais estejam corretas, um token JWT \u00e9 retornado.</li> <li>Consultar: O usu\u00e1rio deve fornecer o token JWT obtido no cadastro ou login (sendo o mesmo em ambos). Caso o token seja v\u00e1lido, a API retorna uma lista de criptomoedas coletadas.</li> </ul>"},{"location":"projeto/main/#21-dockerizacao","title":"2.1 - Dockeriza\u00e7\u00e3o","text":"<p>A aplica\u00e7\u00e3o foi containerizada utilizando o Docker, permitindo que a API seja executada em qualquer ambiente que suporte Docker. O Dockerfile foi configurado para instalar as depend\u00eancias necess\u00e1rias e expor a porta 8000, onde a API est\u00e1 dispon\u00edvel.</p> <p>Al\u00e9m disso, foi criado um arquivo <code>compose.yml</code>, que nele \u00e9 utilizado apenas imagens do Docker Hub para facilitar a execu\u00e7\u00e3o da aplica\u00e7\u00e3o, permitindo que o usu\u00e1rio inicie todos os servi\u00e7os necess\u00e1rios com um \u00fanico comando. Esse arquivo junta duas aplica\u00e7\u00f5es: a Aplica\u00e7\u00e3o FastAPI e o banco de dados Postgres, que armazena os dados dos usu\u00e1rios. Portanto ele se conecta com o banco de dados e realiza as opera\u00e7\u00f5es de CRUD necess\u00e1rias para o funcionamento da API.</p> <p>Utilizando o Docker Desktop, \u00e9 poss\u00edvel visualizar os containers em execu\u00e7\u00e3o, monitorar logs e verificar cria\u00e7\u00e3o dos usu\u00e1rios. Isso facilita o desenvolvimento e a manuten\u00e7\u00e3o da aplica\u00e7\u00e3o.</p>"},{"location":"projeto/main/#22-organizacao-do-projeto","title":"2.2 - Organiza\u00e7\u00e3o do projeto","text":"<p>api/   Dockerfile   app/     main.py   requirements.txt compose.yaml .env .gitignore README.md</p>"},{"location":"projeto/main/#23-arquivos-chaves","title":"2.3 - Arquivos chaves","text":"<ul> <li>compose.yaml: <pre><code>version: '3.9'\n\nservices:\n\n  api:\n    image: minha-api\n    restart: always\n    ports:  \n      - 8000:8000\n    depends_on:\n      - db\n    environment:\n      DATABASE_URL: ${DATABASE_URL:-postgresql://usuario:cloudteste123@db:5432/meu_banco}\n    command: uvicorn app.main:app --host 0.0.0.0 --port 8000\n\n  db:\n    image: postgres\n    hostname: db\n    restart: always\n    environment:\n      POSTGRES_USER: ${MEUUSUARIO:-usuario}\n      POSTGRES_PASSWORD: ${MINHASENHADB:-cloudteste123}\n      POSTGRES_DB: ${POSTGRES_BANCO:-meu_banco}\n</code></pre></li> </ul> <p>OBS: O arquivo <code>.env</code> cont\u00e9m as vari\u00e1veis de ambiente utilizadas no <code>compose.yaml</code>, como <code>DATABASE_URL</code>, <code>MEUUSUARIO</code>, <code>MINHASENHADB</code> e <code>POSTGRES_BANCO</code>. Essas vari\u00e1veis s\u00e3o utilizadas para configurar o banco de dados Postgres e a conex\u00e3o com a API. Para um ambiente de produ\u00e7\u00e3o, \u00e9 importante garantir que essas vari\u00e1veis estejam configuradas corretamente e que as senhas sejam mantidas em segredo, por\u00e9m aqui no projeto, elas est\u00e3o expostas para facilitar o entendimento do funcionamento do projeto.</p>"},{"location":"projeto/main/#24-docker-hub","title":"2.4 - Docker Hub","text":"<p>O projeto foi publicado no Docker Hub, permitindo que qualquer pessoa possa baixar e executar a aplica\u00e7\u00e3o facilmente. O reposit\u00f3rio cont\u00e9m as imagens necess\u00e1rias para executar a aplica\u00e7\u00e3o, facilitando o compartilhamento e a colabora\u00e7\u00e3o entre desenvolvedores.</p> <p>Esse processo de publica\u00e7\u00e3o no Docker Hub foi realizado atrav\u00e9s da cria\u00e7\u00e3o de uma conta no Docker Hub, login na conta atrav\u00e9s do terminal, constru\u00e7\u00e3o da imagem com o comando <code>docker tag minha-api vitorpadova/projeto_cloud</code> e o push da imagem para o reposit\u00f3rio com o comando <code>docker push vitorpadova/projeto_cloud</code>.</p> <p>O reposit\u00f3rio do Docker Hub pode ser acessado atrav\u00e9s do seguinte link: https://hub.docker.com/r/vitorpadova/projeto_cloud.</p>"},{"location":"projeto/main/#31-execucao-do-projeto","title":"3.1 - Execu\u00e7\u00e3o do projeto","text":"<p>Para executar o projeto, siga os seguintes passos: 1. Certifique-se de ter o Docker instalado em sua m\u00e1quina.</p> <ol> <li> <p>Clone o reposit\u00f3rio do projeto: <pre><code>git clone https://github.com/vitorpadovani/cloud_projeto.git\ncd cloud_projeto\n</code></pre></p> </li> <li> <p>Crie um ambiente virtual e ative-o (opcional, mas recomendado): <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\nvenv\\Scripts\\activate  # Windows\n</code></pre></p> </li> <li> <p>Instale as depend\u00eancias do projeto: <pre><code>pip install -r api/requirements.txt # Caso esteja na raiz do projeto\n</code></pre></p> </li> <li> <p>Inicie os containers utilizando o Docker Compose: <pre><code>docker-compose up -d\n</code></pre> Isso iniciar\u00e1 os containers da API e do banco de dados em segundo plano.</p> </li> <li> <p>Acesse a API em <code>http://localhost:8000/docs</code> para visualizar a documenta\u00e7\u00e3o gerada automaticamente pelo FastAPI.</p> </li> <li> <p>Para testar os endpoints, voc\u00ea pode usar ferramentas do pr\u00f3prio <code>/docs</code>. A documenta\u00e7\u00e3o da API tamb\u00e9m fornece exemplos de como fazer as requisi\u00e7\u00f5es.</p> </li> <li> <p>Para parar os containers, utilize o seguinte comando: <pre><code>docker-compose down\n</code></pre> Isso encerrar\u00e1 os containers e liberar\u00e1 os recursos utilizados.</p> </li> </ol>"},{"location":"projeto/main/#32-testes","title":"3.2 - Testes","text":"<p>Os testes das funcionalidades da API foram realizados justamente utilizando a documenta\u00e7\u00e3o gerada pelo FastAPI, que n\u00e3o fornece uma interface mas \u00e9 bom para testar os endpoints. Os testes foram realizados com sucesso, validando o funcionamento correto da API.</p> <p>Os testes realizados foram:</p> <ul> <li>Cadastro de usu\u00e1rio: Verificamos se o usu\u00e1rio foi cadastrado corretamente e se o token JWT foi gerado.</li> </ul> <p></p> <p>Teste Registro de Usu\u00e1rio com sucesso</p> <ul> <li>Cadastro de usu\u00e1rio j\u00e1 existente: Verificamos se o cadastro retornou erro 409 quando o usu\u00e1rio j\u00e1 estava cadastrado.</li> </ul> <p></p> <p>Teste Cadastro de Usu\u00e1rio j\u00e1 existente com erro</p> <ul> <li>Login de usu\u00e1rio: Verificamos se o login foi realizado corretamente e se o token JWT foi gerado.</li> </ul> <p></p> <p>Teste Login de Usu\u00e1rio com sucesso</p> <ul> <li>Login de usu\u00e1rio incorreto: Verificamos se o login retornou erro 401 quando o usu\u00e1rio n\u00e3o estava cadastrado ou se errou o email/senha.</li> </ul> <p></p> <p>Teste Login de Usu\u00e1rio incorreto com erro</p> <ul> <li>Consulta de criptomoedas: Verificamos se a consulta retornou as 10 criptomoedas mais valiosas com sucesso, utilizando o token JWT gerado no cadastro ou login.</li> </ul> <p></p> <p>Teste Consulta de Criptomoedas com sucesso</p> <ul> <li>Consulta de criptomoedas sem autentica\u00e7\u00e3o: Verificamos se a consulta retornou erro 403 quando o usu\u00e1rio n\u00e3o estava autenticado.</li> </ul> <p></p> <p>Teste Consulta de Criptomoedas sem autentica\u00e7\u00e3o</p>"},{"location":"projeto/main/#33-video-de-execucao-da-aplicacao","title":"3.3 - Video de execu\u00e7\u00e3o da aplica\u00e7\u00e3o","text":"<p>O video a seguir mostra a execu\u00e7\u00e3o da aplica\u00e7\u00e3o, com um cadastro de um usu\u00e1rio. Al\u00e9m disso, tamb\u00e9m mostra a execu\u00e7\u00e3o desse usu\u00e1rio entrando na Base de dados e o Docker, portanto, a visualiza\u00e7\u00e3o dos containers em execu\u00e7\u00e3o.</p> <ul> <li>Link do video</li> </ul>"},{"location":"projeto/main/#41-conclusao","title":"4.1 - Conclus\u00e3o","text":"<p>O projeto foi desenvolvido com sucesso, atendendo aos requisitos propostos. A API foi implementada utilizando o FastAPI, permitindo a cria\u00e7\u00e3o de endpoints para cadastro e consulta de criptomoedas. O scraping foi realizado com sucesso, coletando dados atualizados do site CoinMarketCap. A aplica\u00e7\u00e3o foi containerizada com Docker, facilitando a execu\u00e7\u00e3o e portabilidade. Os testes realizados validaram o funcionamento correto da API, garantindo que todas as funcionalidades est\u00e3o operando conforme o esperado. Por fim, o projeto foi publicado no Docker Hub, permitindo que qualquer pessoa possa baixar e executar a aplica\u00e7\u00e3o facilmente.</p> <p>Portanto conseguimos desenvolver conceitos como por exemplo: - Containeriza\u00e7\u00e3o local com Docker Compose - Cria\u00e7\u00e3o de uma API RESTful com FastAPI - Scraping de dados de uma fonte externa - Armazenamento de dados em um banco de dados Postgres - Autentica\u00e7\u00e3o de usu\u00e1rios com JWT - Documenta\u00e7\u00e3o da API com Swagger - Testes de funcionalidades da API utilizando a documenta\u00e7\u00e3o gerada pelo FastAPI</p>"},{"location":"projeto/main/#42-referencias","title":"4.2 - Refer\u00eancias","text":"<ul> <li>Site da Disciplina</li> <li>FastAPI</li> <li>Docker</li> <li>Docker Hub</li> <li>Postgres</li> <li>Video Tutorial Docker Hub</li> <li>CoinMarketCap</li> </ul>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>O principal objetivo deste roteiro \u00e9 aprender a configurar um ambiente de cloud bare-metal. Durante o processo, ser\u00e3o abordados os seguintes pontos:</p> <p>Configura\u00e7\u00e3o de Infraestrutura: Montar a subrede para comunica\u00e7\u00e3o entre os servidores, configurando o ambiente com Ubuntu e utilizando o MaaS para gerenciar o hardware e a rede.</p> <p>Implanta\u00e7\u00e3o e Integra\u00e7\u00e3o de Servi\u00e7os: Realizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o de servi\u00e7os essenciais, como o banco de dados PostgreSQL e a aplica\u00e7\u00e3o Django, incluindo a implementa\u00e7\u00e3o de deploy manual e automatizado com Ansible.</p> <p>Implementa\u00e7\u00e3o de Conectividade e Balanceamento de Carga: Configurar roteadores, DHCP e proxy reverso com NGINX para assegurar a conectividade interna/externa e distribuir a carga entre os servidores.</p>"},{"location":"roteiro1/main/#material-utilizado","title":"Material Utilizado","text":"<p>1 NUC (main) com 10Gb e 1 SSD (120Gb)</p> <p>1 NUC (server1) com 12Gb e 1 SSD (120Gb)</p> <p>1 NUC (server2) com 16Gb e 2 SSD (120Gb+120Gb)</p> <p>3 NUCs (server3, server4 e server5) com 32Gb e 2 SSD (120Gb+120Gb)</p> <p>1 Switch DLink DSG-1210-28 de 28 portas</p> <p>1 Roteador TP-Link TL-R470T+</p>"},{"location":"roteiro1/main/#criando-e-usando-a-infraestrutura","title":"Criando e Usando a Infraestrutura","text":"<p>O objetivo desta etapa \u00e9 preparar a rede f\u00edsica e l\u00f3gica, garantindo que todos os NUCs estejam na mesma subrede e tenham acesso \u00e0 rede externa via roteador. Uma m\u00e1quina principal, chamada de main, ser\u00e1 configurada com o MaaS para gerenciar todas as demais m\u00e1quinas. A seguir, os passos realizados:</p>"},{"location":"roteiro1/main/#1-conexao-dos-dispositivos-e-instalacao-do-ubuntu-server","title":"1. Conex\u00e3o dos Dispositivos e Instala\u00e7\u00e3o do Ubuntu Server","text":"<ul> <li> <p>Conex\u00e3o F\u00edsica:   Todos os NUCs e o roteador s\u00e3o conectados ao switch, formando a base da rede local.</p> </li> <li> <p>Instala\u00e7\u00e3o do Sistema Operacional:   No NUC principal (main), instalamos o Ubuntu Server 22.04.5 LTS para garantir estabilidade e compatibilidade.  </p> </li> <li> <p>Configuramos um IP est\u00e1tico para assegurar o acesso remoto cont\u00ednuo e evitar mudan\u00e7as din\u00e2micas que poderiam comprometer a comunica\u00e7\u00e3o.</p> </li> <li> <p>Instala\u00e7\u00e3o do MaaS:   Utilizamos o MaaS (vers\u00e3o 3.5) para orquestrar e gerenciar o hardware do NUC main.</p> </li> </ul> sudo snap install maas --channel=3.5/Stable <ul> <li>Configura\u00e7\u00e3o Inicial do MaaS: </li> <li>Inicializamos o MaaS com a URL e o banco de dados de teste.  </li> </ul> sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:/// <ul> <li>Criamos o administrador utilizando o login cloud e a senha padr\u00e3o da disciplina.  </li> <li>Habilitamos o acesso remoto via SSH para facilitar a administra\u00e7\u00e3o (veja a se\u00e7\u00e3o de SSH abaixo).  </li> <li>Acessamos o Dashboard do MaaS pelo endere\u00e7o <code>http://172.16.0.3:5240/MAAS</code> e importamos as imagens do Ubuntu (22.04 LTS e 20.04 LTS), para poderem ser utilizadas na instala\u00e7\u00e3o das outras NUCs.  </li> <li>Configuramos o DNS Forwarder para utilizar o DNS externo do Insper.  </li> <li> <p>Em Settings | General, ajustamos os par\u00e2metros do kernel, definindo <code>net.ifnames=0</code>.</p> </li> <li> <p>Configura\u00e7\u00e3o do DHCP no MaaS:</p> </li> <li>Como o switch n\u00e3o tem o servi\u00e7o DHCP, ou seja, ele n\u00e3o consegue entregar IPs aos dispositivos na rede, vamos utilizar o roteador para isso.</li> <li>Habilitamos o DHCP na subrede configurada pelo MaaS Controller e ajustamos o Reserved Range para iniciar em 172.16.11.1 e terminar em 172.16.14.255, dentro da sub-rede definida pela m\u00e1scara 255.255.240.0. Essa configura\u00e7\u00e3o permite que os endere\u00e7os atribu\u00eddos pelo DHCP fiquem restritos a uma parte do espa\u00e7o de endere\u00e7amento dispon\u00edvel, ajudando a controlar e organizar os acessos dentro da rede.  </li> <li> <p>Desabilitamos o DHCP no roteador, para que o MaaS seja o respons\u00e1vel pela distribui\u00e7\u00e3o dos IPs.</p> </li> <li> <p>Verifica\u00e7\u00f5es de Conectividade:   Validamos a configura\u00e7\u00e3o de rede realizando pings para <code>8.8.8.8</code> e <code>www.google.com</code>, assegurando que o roteamento dos pacotes e a resolu\u00e7\u00e3o de URLs estejam funcionando corretamente.</p> </li> </ul>"},{"location":"roteiro1/main/#o-que-e-o-ssh-e-como-ele-foi-configurado","title":"O que \u00e9 o SSH e como ele foi Configurado","text":"<p>SSH (Secure Shell) \u00e9 um protocolo que permite o acesso remoto seguro a sistemas, criando um canal criptografado entre o cliente e o servidor. Isso garante que as informa\u00e7\u00f5es transmitidas, como comandos e credenciais, estejam protegidas contra intercepta\u00e7\u00f5es.</p> <ol> <li> <p>Gera\u00e7\u00e3o do Par de Chaves:    Utilizamos <code>ssh-keygen -t -rsa</code> para criar um par de chaves (p\u00fablica e privada), permitindo a autentica\u00e7\u00e3o sem a necessidade de enviar senhas em texto claro.</p> </li> <li> <p>Distribui\u00e7\u00e3o da Chave P\u00fablica:    A chave p\u00fablica gerada foi copiada para o servidor (NUC main), possibilitando que o servidor autentique o cliente que possui a chave privada correspondente. Vale ressaltar que esse servi\u00e7o do ssh trabalha na porta 22.</p> </li> </ol>"},{"location":"roteiro1/main/#3-comissionamento-dos-servidores-e-criacao-de-ovs-bridge","title":"3. Comissionamento dos Servidores e Cria\u00e7\u00e3o de OVS Bridge","text":"<ul> <li>Comissionamento dos Servidores:   No Dashboard do MaaS, cadastramos os hosts (server1 at\u00e9 server5) e configuramos a op\u00e7\u00e3o de Power Type para Intel AMT. Foram inseridos os seguintes detalhes:  </li> <li>MAC Address (anotado previamente).  </li> <li>Senha padr\u00e3o: <code>CloudComp6s!</code>.  </li> <li>IP do AMT, no formato <code>172.16.15.X</code> (onde X corresponde ao id do servidor, por exemplo, server1 = 172.16.15.1).   Ap\u00f3s o boot via PXE, os servidores devem aparecer com o status \"Ready\", indicando que os par\u00e2metros de hardware (CPU, mem\u00f3ria, SSD e rede) foram detectados corretamente.   Tamb\u00e9m adicionamos o roteador como device no Dashboard do MaaS.</li> </ul> <p>Obs: O server 1 da nossa cloud estava enfrentando problemas para ser encontrado. Como esse roteiro n\u00e3o necessitava a utiliza\u00e7\u00e3o de todos os servers, seguimos utilizando os demais.</p> <ul> <li>Cria\u00e7\u00e3o de OVS Bridge:   Para reduzir a necessidade de duas interfaces de rede f\u00edsicas, configuramos uma Open vSwitch (OVS) bridge.  </li> <li>A ponte \u00e9 criada a partir da interface padr\u00e3o <code>enp1s0</code> e nomeada br-ex.  </li> <li>Essa configura\u00e7\u00e3o \u00e9 aplicada em todos os cinco n\u00f3s, garantindo flexibilidade e suporte ao OVN Chassis.</li> </ul>"},{"location":"roteiro1/main/#4-configuracao-de-nat-e-acesso-remoto","title":"4. Configura\u00e7\u00e3o de NAT e Acesso Remoto","text":""},{"location":"roteiro1/main/#o-que-e-nat","title":"O que \u00e9 NAT?","text":"<p>O NAT (Network Address Translation) \u00e9 uma t\u00e9cnica que permite que dispositivos de uma rede privada compartilhem um \u00fanico endere\u00e7o IP p\u00fablico para acessar a internet. Ele realiza a tradu\u00e7\u00e3o dos endere\u00e7os IP privados para um endere\u00e7o p\u00fablico e vice-versa, garantindo que os pacotes de dados sejam direcionados corretamente entre a rede interna e a externa.</p>"},{"location":"roteiro1/main/#como-o-nat-funciona-na-nossa-configuracao","title":"Como o NAT Funciona na Nossa Configura\u00e7\u00e3o","text":"<ul> <li> <p>Tradu\u00e7\u00e3o de Endere\u00e7os:   Cada dispositivo na rede local possui um endere\u00e7o IP privado (por exemplo, na faixa 172.16.0.0/20). Quando um dispositivo envia dados para a internet, o roteador que realiza o NAT substitui o endere\u00e7o IP privado pelo endere\u00e7o IP p\u00fablico configurado. Dessa forma, o tr\u00e1fego de sa\u00edda parece originar de um \u00fanico IP p\u00fablico.</p> </li> <li> <p>Port Forwarding para Acesso Remoto:   Para permitir a conex\u00e3o remota ao servidor principal (main) pela porta 22, configuramos um redirecionamento de porta (port forwarding). Isso faz com que qualquer conex\u00e3o que chegue \u00e0 porta 22 do endere\u00e7o IP p\u00fablico seja encaminhada para o IP fixo do servidor main (172.16.0.3).</p> </li> </ul>"},{"location":"roteiro1/main/#configuracao-do-nat-e-das-regras-de-gerenciamento-no-roteador","title":"Configura\u00e7\u00e3o do NAT e das Regras de Gerenciamento no Roteador","text":"<ul> <li> <p>Acesso \u00e0 Interface do Roteador:   A configura\u00e7\u00e3o \u00e9 realizada atrav\u00e9s da interface administrativa do roteador, acess\u00edvel via navegador atrav\u00e9s do IP do dispositivo.</p> </li> <li> <p>Defini\u00e7\u00e3o das Regras de NAT:   Na interface do roteador, configuramos as seguintes regras:</p> <ol> <li>Regra de NAT: Define que todos os dispositivos da subrede (172.16.0.0/20) usem o endere\u00e7o IP p\u00fablico do roteador para acessar a internet.</li> <li> <p>Port Forwarding: Cria uma regra para redirecionar conex\u00f5es que chegam na porta 22 para o IP 172.16.0.3, permitindo o acesso remoto seguro ao servidor main.</p> </li> <li> <p>Regras de Gerenciamento Remoto: Adicionalmente, foi criada uma regra que permite o acesso remoto ao pr\u00f3prio roteador (regra de gest\u00e3o), configurada para aceitar conex\u00f5es de qualquer endere\u00e7o (0.0.0.0/0). Essa regra possibilita a administra\u00e7\u00e3o do roteador remotamente.</p> </li> </ol> </li> </ul> <p>Essa configura\u00e7\u00e3o garante que a rede local tenha acesso \u00e0 internet por meio de um \u00fanico endere\u00e7o IP p\u00fablico, ao mesmo tempo que permite o gerenciamento remoto seguro tanto do servidor main quanto do roteador.</p>"},{"location":"roteiro1/main/#bare-metal-aplicacao","title":"Bare Metal - Aplica\u00e7\u00e3o","text":"<p>Nesta etapa, realizamos o deploy manual de uma aplica\u00e7\u00e3o simples em Django utilizando a infraestrutura configurada anteriormente na nuvem MaaS. \u00c9 importante notar que, devido a problemas t\u00e9cnicos, o server originalmente designado como \"server1\" n\u00e3o funcionou corretamente. Dessa forma, o servidor que inicialmente seria o server1 passou a ser considerado como server2, o que acarretou um reajuste na numera\u00e7\u00e3o dos demais servidores (server2 passou a ser server3, e assim por diante).</p> <p>Al\u00e9m disso, durante a instala\u00e7\u00e3o das imagens nos servidores, encontramos um problema inesperado: a Canonical atualizou a imagem do Ubuntu, alterando algum componente essencial e rompendo a compatibilidade. Para resolver esse problema, foi necess\u00e1rio atualizar o firmware em todas as m\u00e1quinas para que o Ubuntu 22.04 funcionasse corretamente.</p>"},{"location":"roteiro1/main/#1-ajuste-no-dns-do-servidor","title":"1. Ajuste no DNS do Servidor","text":"<p>Antes de iniciar o deploy, foi preciso ajustar a configura\u00e7\u00e3o do DNS:</p> <ul> <li>Acesse a aba Subnets no MaaS e edite a subnet <code>172.16.0.0/20</code>, alterando o campo Subnet summary para usar o DNS do Insper (<code>172.20.129.131</code>).</li> </ul>"},{"location":"roteiro1/main/#2-primeira-parte-configuracao-do-banco-de-dados","title":"2. Primeira Parte: Configura\u00e7\u00e3o do Banco de Dados","text":"<p>Utilizamos o PostgreSQL, um servidor de banco de dados robusto e amplamente utilizado em projetos open source, conforme os passos abaixo:</p> <ul> <li>Deploy do Ubuntu:   No MaaS, foi realizado o deploy do Ubuntu 22.04 no servidor designado (que originalmente seria o server1, mas considerando o ajuste, este \u00e9 agora o server2).</li> <li>Instala\u00e7\u00e3o e Configura\u00e7\u00e3o do PostgreSQL:   No terminal do servidor (acessado via SSH), executamos os seguintes procedimentos:</li> <li>Atualiza\u00e7\u00e3o do sistema.</li> </ul> sudo apt update <ul> <li>Instala\u00e7\u00e3o dos pacotes do PostgreSQL e suas contribui\u00e7\u00f5es.</li> </ul> sudo apt install postgresql postgresql-contrib -y <ul> <li>Cria\u00e7\u00e3o de um usu\u00e1rio para a aplica\u00e7\u00e3o (usu\u00e1rio <code>cloud</code> com senha <code>cloud</code>).</li> </ul> sudo su - postgrescreateuser -s cloud -W <ul> <li>Cria\u00e7\u00e3o de uma base de dados (por exemplo, <code>tasks</code>).</li> </ul> createdb -O cloud tasks <ul> <li>Edi\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o do PostgreSQL para que o servi\u00e7o aceite conex\u00f5es remotas (definindo <code>listen_addresses = '*'</code>).</li> </ul> nano /etc/postgresql/14/main/postgresql.conf <ul> <li>Ajuste no arquivo de controle de acesso (<code>pg_hba.conf</code>) para liberar conex\u00f5es vindas da subnet <code>172.16.0.0/20</code>.</li> <li>Libera\u00e7\u00e3o da porta (5432) no firewall e reinicializa\u00e7\u00e3o do servi\u00e7o.</li> </ul> sudo ufw allow 5432/tcpsudo systemctl restart postgresql"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Verificando se o banco est\u00e1 funcionando</p> <p></p> <p>Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional</p> <p></p> <p>Verifica se iniciou sem erro </p> <p></p> <p>Acessivel na pr\u00f3pria maquina na qual ele foi implantado</p> <p></p> <p>Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN na porta 5432.</p>"},{"location":"roteiro1/main/#3-parte-ii-deploy-da-aplicacao-django","title":"3. Parte II: Deploy da Aplica\u00e7\u00e3o Django","text":"<ul> <li>Reserva e Deploy da M\u00e1quina:   Utilizando o MaaS CLI, reservamos uma m\u00e1quina (originalmente designada como server2, mas, com o ajuste, ela se torna o novo server3 conforme a sequ\u00eancia), e realizamos o deploy atrav\u00e9s do CLI do MaaS. Vale ressaltar que, por conta do problema com a imagem, tivemos que refazer todos os deploys manualmente pelo MaaS.</li> <li>Clone e Instala\u00e7\u00e3o da Aplica\u00e7\u00e3o:   Acessamos o servidor via SSH, clonamos o reposit\u00f3rio da aplica\u00e7\u00e3o Django e executamos o script de instala\u00e7\u00e3o (<code>install.sh</code>).</li> </ul> maas [login] machine deploy [system_id]git clone https://github.com/raulikeda/tasks.git./install.shsudo reboot <ul> <li> <p>Instalando e configurando o Django (Parte da Tarefa 3):</p> <p>Ap\u00f3s o deploy manual da nossa m\u00e1quina, por conta do erro, para baixar a aplica\u00e7\u00e3o do Django, entramos no server3 e rodamos os seguintes comandos para instalar o Django:</p> <p>sudo apt install python3-psycopg2pip install django --break-system-packages </p> <p>Al\u00e9m disso, para a aplica\u00e7\u00e3o funcionar, modificamos o arquivo <code>settings.py</code> do Django para indicar que o banco de dados que iremos utilizar estar\u00e1 no server2 e ajustamos os <code>ALLOWED_HOSTS</code> para permitir o acesso sem problemas a partir do main.</p> <p>nano portfolio/settings.py </p> </li> <li> <p>Fazendo com que o server3 Enxergue o server2</p> <p>Como apenas o servidor main consegue visualizar todas as m\u00e1quinas (devido \u00e0 sua fun\u00e7\u00e3o de controlador do MaaS), precisamos informar o server3 sobre a localiza\u00e7\u00e3o do server2, que hospeda o PostgreSQL para a aplica\u00e7\u00e3o Django. Para isso, editamos o arquivo <code>/etc/hosts</code> no server3 e adicionamos uma entrada que associa o nome (\"server2\") ao respectivo endere\u00e7o IP.</p> <p>Essa a\u00e7\u00e3o garante que, mesmo sem acesso direto \u00e0 tabela de DNS da rede, o server3 consiga resolver o nome do server2 e estabelecer comunica\u00e7\u00e3o com ele. Al\u00e9m disso, desabilitamos a configura\u00e7\u00e3o para que essa altera\u00e7\u00e3o n\u00e3o seja perdida ap\u00f3s uma reinicializa\u00e7\u00e3o.</p> <p>sudo nano /etc/hosts </p> </li> </ul> <p>Teste da Aplica\u00e7\u00e3o:   Ap\u00f3s reiniciar a m\u00e1quina, testamos o acesso \u00e0 aplica\u00e7\u00e3o via terminal do MaaS utilizando um comando para verificar a porta 8080.</p> wget http://172.16.0.15:8080/admin/ <p>Para acesso no browser, criamos um t\u00fanel SSH redirecionando a porta 8080 do servidor para a porta 8001 local.</p> ssh cloud@10.103.1.19 -L 8001:172.16.0.15:8080 <p>Esse t\u00fanel faz com que utilizemos a porta 8001 do computador local para acessar o que est\u00e1 na porta 8080 do server3. Isso significa que qualquer solicita\u00e7\u00e3o feita para http://localhost:8001/admin/ ser\u00e1 redirecionada para http://172.16.0.15:8080/admin/. Dessa forma, podemos acessar a interface de administra\u00e7\u00e3o do django do server3 localmente atrav\u00e9s do navegador web.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<p>Dashboard do MAAS com Server 2 e 3 deployados</p> <p></p> <p>Comprova\u00e7\u00e3o das imagens do Ubuntu sincronizadas</p> <p> </p> <p>Todas as m\u00e1quinas passaram nos testes de hardware e commissioning com status \"OK\"</p>"},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<p>Dashboard do MAAS com os IPS das m\u00e1quinas</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server3</p> <p></p> <p>Comprovando que estamos rodando no server3</p>"},{"location":"roteiro1/main/#4-deploy-automatizado-com-ansible","title":"4. Deploy Automatizado com Ansible","text":"<p>O Ansible \u00e9 uma ferramenta de automa\u00e7\u00e3o que permite gerenciar configura\u00e7\u00f5es e implanta\u00e7\u00f5es de forma padronizada e repet\u00edvel. Diferente do processo manual, ele garante que os passos de instala\u00e7\u00e3o sejam executados da mesma forma em todas as m\u00e1quinas, sem interferir em estados intermedi\u00e1rios \u2013 um conceito conhecido como idempot\u00eancia.</p>"},{"location":"roteiro1/main/#o-que-acontece-durante-o-deploy-automatizado","title":"O Que Acontece Durante o Deploy Automatizado?","text":"<ol> <li> <p>Provisionamento da M\u00e1quina de Destino:    Ap\u00f3s solicitar o deploy via MaaS, uma nova m\u00e1quina (agora designada como server4) \u00e9 alocada para a aplica\u00e7\u00e3o.</p> </li> <li> <p>Instala\u00e7\u00e3o do Ansible no Servidor Principal:    Instalamos o Ansible no servidor principal (main). Isso transforma o main em um controlador central que se conecta \u00e0s m\u00e1quinas remotas para executar os comandos necess\u00e1rios.</p> </li> </ol> sudo apt install ansible <ol> <li>Baixar o Playbook:    Um playbook \u00e9 um arquivo YAML que cont\u00e9m uma s\u00e9rie de instru\u00e7\u00f5es que definem como a aplica\u00e7\u00e3o Django deve ser instalada e configurada. Ao baixar esse arquivo, garantimos que todos os passos necess\u00e1rios ser\u00e3o executados de forma padronizada.</li> </ol> wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml <ol> <li>Execu\u00e7\u00e3o do Playbook:    Com o playbook em m\u00e3os, utilizamos o comando do Ansible para execut\u00e1-lo, passando o IP do server4 como vari\u00e1vel extra. Esse processo automatiza a instala\u00e7\u00e3o da aplica\u00e7\u00e3o Django, realizando as mesmas a\u00e7\u00f5es que seriam feitas manualmente (como instala\u00e7\u00e3o de pacotes, configura\u00e7\u00e3o de servi\u00e7os e deploy da aplica\u00e7\u00e3o), mas sem a necessidade de interven\u00e7\u00e3o manual em cada servidor.</li> </ol> ansible-playbook tasks-install-playbook.yaml --extra-vars server=172.16.0.17 <ol> <li> <p>Fazendo com que o server4 enxergue o server2</p> <p>Seguindo o mesmo processo feito para o server3, devemos fazer para o server4 enxergar o server2. Para isso entramos nas configura\u00e7\u00d5es do <code>/etc/hosts</code> no server3 e adicionamos uma entrada que associa o nome (\"server2\") ao respectivo endere\u00e7o IP.</p> </li> </ol>"},{"location":"roteiro1/main/#por-que-usar-o-ansible","title":"Por Que Usar o Ansible?","text":"<ul> <li> <p>Idempot\u00eancia:   O Ansible garante que, mesmo se executarmos o playbook m\u00faltiplas vezes, o sistema sempre alcan\u00e7ar\u00e1 o estado desejado, sem efeitos colaterais indesejados.</p> </li> <li> <p>Gerenciamento Simult\u00e2neo de V\u00e1rias M\u00e1quinas:   Permite que um \u00fanico playbook seja aplicado a m\u00faltiplos servidores de uma vez, garantindo uniformidade na instala\u00e7\u00e3o e configura\u00e7\u00e3o da aplica\u00e7\u00e3o.</p> </li> <li> <p>Facilidade de Automa\u00e7\u00e3o:   Com um arquivo de configura\u00e7\u00e3o centralizado, qualquer altera\u00e7\u00e3o ou corre\u00e7\u00e3o pode ser aplicada de maneira r\u00e1pida e consistente em todos os n\u00f3s.</p> </li> <li> <p>Redu\u00e7\u00e3o de Erros Humanos:   Automatizando o processo, minimizamos a chance de erros que podem ocorrer durante a configura\u00e7\u00e3o manual de cada servidor.</p> </li> </ul>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<p>Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server3</p> <p></p> <p>Comprovando que estamos rodando no server3</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server4</p> <p></p> <p>Comprovando que estamos rodando no server4</p> <p>A diferen\u00e7a entre instalar manualmente a aplica\u00e7\u00e3o Django e utilizar o Ansible \u00e9 que, na instala\u00e7\u00e3o manual, cada comando deve ser executado individualmente em cada servidor, o que aumenta o risco de erros e pode resultar em configura\u00e7\u00f5es inconsistentes. J\u00e1 com o Ansible, um playbook automatiza todo o processo, garantindo que os mesmos passos sejam aplicados de forma id\u00eantica em todos os n\u00f3s, tornando o deploy mais r\u00e1pido, confi\u00e1vel e escal\u00e1vel, al\u00e9m de reduzir significativamente a possibilidade de erros humanos.</p>"},{"location":"roteiro1/main/#5-balancamento-de-carga-com-proxy-reverso","title":"5. Balancamento de Carga com Proxy Reverso","text":"<p>O balanceamento de carga com proxy reverso tem como objetivo centralizar o acesso \u00e0 aplica\u00e7\u00e3o, de forma que uma \u00fanica entrada redirecione as requisi\u00e7\u00f5es para v\u00e1rios servidores que hospedam a aplica\u00e7\u00e3o Django. Essa abordagem \u00e9 essencial para garantir alta disponibilidade e redund\u00e2ncia: se um dos servidores falhar, os outros continuam atendendo as requisi\u00e7\u00f5es, mantendo a estabilidade do servi\u00e7o.</p> <p>Nesse cen\u00e1rio, instalamos o NGINX no server5 para atuar como proxy reverso. O primeiro passo \u00e9 instalar o NGINX, utilizando o gerenciador de pacotes do sistema:</p> sudo apt-get install nginx <p>Ap\u00f3s a instala\u00e7\u00e3o, editamos o arquivo de configura\u00e7\u00e3o do NGINX, localizado em <code>/etc/nginx/sites-available/default</code>. Nele, definimos o bloco upstream que aponta para os servidores backend onde a aplica\u00e7\u00e3o Django est\u00e1 rodando:</p> <pre><code>upstream backend { server 172.16.0.15:8080; server 172.16.0.17:8080; }\n</code></pre> <p>Em seguida, configuramos o servidor virtual para encaminhar todas as requisi\u00e7\u00f5es que chegarem na porta 80 para o grupo de servidores definido acima, utilizando a diretiva <code>proxy_pass</code>:</p> <pre><code>server { location / { proxy_pass http://backend; } }\n</code></pre> <p>Al\u00e9m disso comentamos algumas linhas para que n\u00e3o houvesse conflitos das informa\u00e7\u00d5es.</p> <p>Ap\u00f3s todas as configura\u00e7\u00f5es realizadas no nginx, reiniciamos o servi\u00e7o</p> sudo service nginx restart <p>A partir deste momento, o NGINX distribuir\u00e1 as requisi\u00e7\u00f5es recebidas entre os servidores definidos no bloco upstream, utilizando o algoritmo Round Robin por padr\u00e3o. Isso melhora a escalabilidade e a toler\u00e2ncia a falhas, pois se um dos servidores ficar indispon\u00edvel, o outro continuar\u00e1 atendendo \u00e0s requisi\u00e7\u00f5es.</p> <p>Para verificar o funcionamento, modificamos a fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> em cada inst\u00e2ncia da aplica\u00e7\u00e3o Django, atribuindo mensagens distintas. Assim, ao acessar a aplica\u00e7\u00e3o por meio do server5, podemos observar que as respostas alternam entre as mensagens, confirmando que o balanceamento de carga est\u00e1 ativo e direcionando as requisi\u00e7\u00f5es para cada servidor de forma rotativa.</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<p>Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs</p> <p></p> <p>Mudan\u00e7a da mensagem no views.py server3</p> <p></p> <p>Mudan\u00e7a da mensagem no views.py server4</p> <p></p> <p>Requisi\u00e7\u00e3o com print da mensagem do server3</p> <p></p> <p>Requisi\u00e7\u00e3o com print da mensagem do server4</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Durante a configura\u00e7\u00e3o e implementa\u00e7\u00e3o da nuvem bare-metal, surgiram diversos pontos de discuss\u00e3o e aprendizado:</p> <ul> <li> <p>Configura\u00e7\u00e3o de Rede e MaaS: Embora o MaaS simplifique a orquestra\u00e7\u00e3o de hardware, entender sua l\u00f3gica de DHCP e a intera\u00e7\u00e3o com o roteador demandou um tempo consider\u00e1vel. O fato de o switch n\u00e3o possuir DHCP embutido exigiu aten\u00e7\u00e3o redobrada na defini\u00e7\u00e3o dos endere\u00e7os IP e no desvio de fun\u00e7\u00f5es do roteador para o MaaS.  </p> </li> <li> <p>Problemas T\u00e9cnicos e Solu\u00e7\u00f5es: A indisponibilidade do servidor que originalmente seria o server1 mostrou a import\u00e2ncia de ter planos de conting\u00eancia. O ajuste na numera\u00e7\u00e3o dos servidores e a atualiza\u00e7\u00e3o de firmware para corrigir a compatibilidade com o Ubuntu 22.04 demonstram como ambientes de produ\u00e7\u00e3o podem exigir adapta\u00e7\u00f5es e reconfigura\u00e7\u00f5es inesperadas.</p> </li> <li> <p>Instala\u00e7\u00e3o Manual vs. Ansible: A instala\u00e7\u00e3o manual do Django e do banco de dados PostgreSQL, embora did\u00e1tica para fins de aprendizado, revelou-se propensa a erros e repeti\u00e7\u00f5es. Em contrapartida, o uso do Ansible permitiu automatizar a configura\u00e7\u00e3o de m\u00faltiplos servidores de forma consistente, evidenciando a escalabilidade e a confiabilidade proporcionadas pelas ferramentas de automa\u00e7\u00e3o.</p> </li> <li> <p>Balanceamento de Carga: Configurar o NGINX como proxy reverso enfatizou a import\u00e2ncia de uma \u00fanica entrada de acesso para distribuir requisi\u00e7\u00f5es entre v\u00e1rios servidores. Esse passo \u00e9 fundamental para garantir alta disponibilidade, especialmente em cen\u00e1rios de produ\u00e7\u00e3o que exigem redund\u00e2ncia e escalabilidade.</p> </li> <li> <p>Dificuldades e Facilidade: A maior dificuldade relatada foi lidar com ajustes de firmware e vers\u00f5es de imagem que mudaram subitamente. J\u00e1 a maior facilidade percebida foi a automa\u00e7\u00e3o do deploy com Ansible, que simplificou a replica\u00e7\u00e3o das configura\u00e7\u00f5es e reduziu erros humanos.</p> </li> </ul>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro proporcionou uma vis\u00e3o completa do processo de cria\u00e7\u00e3o e gest\u00e3o de uma nuvem bare-metal. Iniciando pela configura\u00e7\u00e3o da infraestrutura de rede e passando pela instala\u00e7\u00e3o manual de servi\u00e7os cr\u00edticos como o PostgreSQL e o Django, o aprendizado foi consolidado ao automatizar tarefas com Ansible e ao implementar um balanceador de carga via NGINX.</p> <p>No final, foi poss\u00edvel concluir que:</p> <ol> <li> <p>Gerenciamento Centralizado \u00e9 Essencial: O MaaS demonstrou ser uma ferramenta valiosa para orquestrar o provisionamento e o controle de m\u00faltiplos servidores, otimizando a aloca\u00e7\u00e3o de recursos e o gerenciamento de IPs.</p> </li> <li> <p>Automa\u00e7\u00e3o Reduz Erros e Garante Consist\u00eancia: A diferen\u00e7a entre instalar servi\u00e7os manualmente e utilizar o Ansible evidenciou o quanto a automa\u00e7\u00e3o traz confiabilidade, escalabilidade e facilidade de manuten\u00e7\u00e3o.</p> </li> <li> <p>Alta Disponibilidade e Redund\u00e2ncia: A configura\u00e7\u00e3o de um proxy reverso (NGINX) para balanceamento de carga exemplificou a import\u00e2ncia de garantir que a aplica\u00e7\u00e3o se mantenha dispon\u00edvel mesmo diante de falhas pontuais em um ou mais servidores.</p> </li> </ol> <p>Em suma, este roteiro possibilitou a experi\u00eancia pr\u00e1tica de construir uma infraestrutura de cloud bare-metal, destacando desde os fundamentos de rede at\u00e9 a entrega de aplica\u00e7\u00f5es de forma robusta e escal\u00e1vel, aproximando o aprendizado dos desafios reais encontrados em data centers e ambientes de produ\u00e7\u00e3o.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#objetivo","title":"Objetivo","text":"<p>Este roteiro tem como objetivo apresentar os fundamentos do Juju como uma plataforma de orquestra\u00e7\u00e3o de deployment para aplica\u00e7\u00f5es distribu\u00eddas. </p> <p>O foco est\u00e1 em compreender como o Juju simplifica a configura\u00e7\u00e3o e o gerenciamento de servi\u00e7os. Al\u00e9m disso, ser\u00e3o abordados os conceitos de comunica\u00e7\u00e3o entre aplica\u00e7\u00f5es e servi\u00e7os, destacando como o Juju automatiza a intera\u00e7\u00e3o entre os componentes de um sistema distribu\u00eddo.</p> <p>Para produ\u00e7\u00e3o desse roteiro foi necess\u00e1ria a conclus\u00e3o da etapa do Bare Metal (Roteiro 1) garantindo que a infraestrutura de base esteja configurada corretamente antes de seguir com a implementa\u00e7\u00e3o do Juju.</p>"},{"location":"roteiro2/main/#material-utilizado","title":"Material Utilizado","text":"<p>1 NUC (main) com 10Gb e 1 SSD (120Gb)</p> <p>1 NUC (server1) com 12Gb e 1 SSD (120Gb)</p> <p>1 NUC (server2) com 16Gb e 2 SSD (120Gb+120Gb)</p> <p>3 NUCs (server3, server4 e server5) com 32Gb e 2 SSD (120Gb+120Gb)</p> <p>1 Switch DLink DSG-1210-28 de 28 portas</p> <p>1 Roteador TP-Link TL-R470T+   </p>"},{"location":"roteiro2/main/#criando-a-infraestrutura","title":"Criando a Infraestrutura","text":"<p>Nesta etapa, configuraremos o Juju para gerenciar a infraestrutura de servidores anteriormente criadas e configuradas e que est\u00e3o sendo gerenciados pelo MAAS.</p> <p>Diferente do Ansible, que utilizamos para automatizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o de servi\u00e7os no roteiro anterior, o Juju consegue integrar-se diretamente ao MAAS, atuando como um orquestrador de deploy que provisiona e gerencia os recursos computacionais dinamicamente.</p> <p>Fizemos o release do Postgres e Django do roteiro anterior, e iniciamos pela instala\u00e7\u00e3o do Juju.</p>"},{"location":"roteiro2/main/#1-instalacao-e-configuracao-do-juju","title":"1. Instala\u00e7\u00e3o e configura\u00e7\u00e3o do Juju","text":"<p>Utilizamos o Juju 3.6 para este roteiro, que ser\u00e1 utilizado para gerenciar a infraestrutura de servidores.</p> <pre><code>sudo snap install juju --channel 3.6\n</code></pre> <p>O Juju utilizar\u00e1 o MAAS para provedor de m\u00e1quinas e sistema operacional, ou seja, haver\u00e1 uma integra\u00e7\u00e3o entre o Juju e o MAAS. Para isso, foi necess\u00e1rio configur\u00e1-lo para que possa enxergar o MAAS. Foi feita a cria\u00e7\u00e3o de dois arquivos para defini\u00e7\u00e3o do cloud e do credential.</p> <p>Primeiro criamos o arquivo <code>maas-cloud.yaml</code> que informa ao Juju onde encontrar o servi\u00e7o MAAS e como ele deve se autenticar. Utilizamos o endere\u00e7o local e sua porta padr\u00e3o para conex\u00e3o que \u00e9 5240.</p> <pre><code>clouds:\n    maas-one:\n        type: maas\n        auth-types: [oauth1]\n        endpoint: http://172.16.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, importamos o arquivo de configura\u00e7\u00e3o para o Juju.</p> <pre><code>juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>Depois, criamos o arquivo <code>maas-creds.yaml</code> que cont\u00e9m as credenciais de acesso ao MAAS. Necess\u00e1rio para que o Juju gerencie as m\u00e1quinas dentro da cloud definida</p> <p><pre><code>credentials:\n    maas-one:\n    anyuser:\n        auth-type: oauth1\n        maas-oauth: &lt;API KEY&gt;\n</code></pre> OBS: API KEY \u00e9 a chave que permite a autentica\u00e7\u00e3o, gerada dentro do MAAS, no menu do usu\u00e1rio</p> <p>Em seguida, importamos o arquivo de credenciais para o Juju.</p> <pre><code>juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#2-criando-o-controller-do-juju","title":"2. Criando o controller do Juju","text":"<p>O Juju precisa de um controller, uma m\u00e1quina exclusiva para gerenciar a orquestra\u00e7\u00e3o dos servi\u00e7os. Al\u00e9m disso, ele \u00e9 o componente central que gerencia o ciclo de vida das aplica\u00e7\u00f5es. Ele \u00e9 respons\u00e1vel por alocar m\u00e1quinas, monitorar os servi\u00e7os e gerenciar o deploy de novas aplica\u00e7\u00f5es.</p> <p>Para isso, utilizaremos o server1, que foi marcado com a tag juju dentro do painel do MAAS.</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>O par\u00e2metro <code>--bootstrap-series=jammy</code> define a vers\u00e3o do sistema operacional que ser\u00e1 utilizada (Ubuntu 22.04). O par\u00e2metro <code>--constraints tags=juju</code> define que o controller ser\u00e1 criado na m\u00e1quina que possui a tag juju.</p> <p>O comando acima demora um pouco para ser executado, porque diversas etapas s\u00e3o realizadas para configurar o controller, como:</p> <ul> <li> <p>Provisionamento da m\u00e1quina no MAAS</p> </li> <li> <p>Instala\u00e7\u00e3o do Juju</p> </li> <li> <p>Configura\u00e7\u00e3o do Juju para se conectar ao MAAS</p> </li> <li> <p>Testes de conectividade, e verifica\u00e7\u00f5es finais</p> </li> </ul> <p>Por fim criamos o modelo 'openstack', que \u00e9 o ambiente onde as aplica\u00e7\u00f5es ser\u00e3o instaladas.</p> <p>Os modelos s\u00e3o ambientes isolados dentro de um controller, usados para organizar diferentes aplica\u00e7\u00f5es</p> <pre><code>juju add-model --config default-series=jammy openstack\n</code></pre>"},{"location":"roteiro2/main/#utilizando-a-infraestrutura","title":"Utilizando a Infraestrutura","text":"<p>Com o controller do Juju configurado, podemos come\u00e7ar a utilizar a infraestrutura de servidores para o deployment de aplica\u00e7\u00f5es. </p> <p>Nesta etapa, trabalharemos com duas aplica\u00e7\u00f5es essenciais para monitoramento: Grafana e Prometheus. Sendo o Grafana respons\u00e1vel por simplificar a apresenta\u00e7\u00e3o visual de dados, e o Prometheus por coletar e armazenar essas m\u00e9tricas, funcionando como um banco de dados. </p> <p>Com ambas instaladas, ser\u00e1 poss\u00edvel acompanhar o desempenho e a integridade dos servi\u00e7os em execu\u00e7\u00e3o atrav\u00e9s de um dashboard.</p>"},{"location":"roteiro2/main/#1-deploy-do-dashboard-do-juju","title":"1. Deploy do Dashboard do Juju","text":"<p>O primeiro passo foi instalar o dashboard do Juju, para isso, voltamos para o controller do Juju para instalar o dashboard.</p> <pre><code>juju switch controller\n</code></pre> <p>Al\u00e9m disso, criamos a tag dashboard-juju no MAAS para que o Juju pudesse instalar o dashboard na m\u00e1quina correta. Essa tag foi atribu\u00edda ao server3 para o deploy do dashboard e utilizada com o par\u00e2metro <code>--constraints tags=dashboard-juju</code>.</p> <pre><code>juju deploy juju-dashboard dashboard --constraints tags=dashboard-juju\n</code></pre> <p>Ap\u00f3s o deploy, conseguimos integrar o controller com o dashboard do Juju e expor o servi\u00e7o para acesso externo.</p> <pre><code>juju integrate dashboard controller\n</code></pre> <pre><code>juju expose dashboard\n</code></pre> <p>Agora, nosso servi\u00e7o de dashboard do Juju est\u00e1 dispon\u00edvel para acesso externo. Com esse c\u00f3digo, obtivemos o endere\u00e7o e as credenciais para acesso.</p> <pre><code>juju dashboard\n</code></pre> <p>Al\u00e9m disso, foi necess\u00e1rio criar um t\u00fanel SSH para acessar o dashboard do Juju remotamente. O comando abaixo cria uma conex\u00e3o segura entre o nosso localhost e o server3, onde o dashboard do Juju est\u00e1 rodando.</p> <pre><code>ssh cloud@10.103.1.19 -L 31666:172.16.0.22:8080\n\n\n### 2. Deploy do Grafana e Prometheus\n\nCriamos uma pasta para armazenar os charms que ser\u00e3o utilizados no deploy das aplica\u00e7\u00f5es.\n\nOBS: **Charms** s\u00e3o pacotes de software que cont\u00eam tudo o que \u00e9 necess\u00e1rio para executar um servi\u00e7o. Eles funcionam como scripts encapsulados que definem como uma aplica\u00e7\u00e3o deve ser instalada, gerenciada e integrada com outras\n\n``` bash\nmkdir -p /home/cloud/charms\ncd /home/cloud/charms\n</code></pre> <p>Em seguida, baixamos os charms do Grafana e do Prometheus.</p> <pre><code>juju download grafana\n</code></pre> <pre><code>juju download prometheus2\n</code></pre> <p>O deploy das aplica\u00e7\u00f5es foi feito com auxilio do Juju, utilizando o modelo 'openstack' criado anteriormente. Isso foi importante para que n\u00e3o sobrecarreg\u00e1ssemos o controller com a instala\u00e7\u00e3o de aplica\u00e7\u00f5es em um \u00fanico modelo.</p> <p>Al\u00e9m disso, fizemos o deploy do Prometheus e do Grafana em m\u00e1quinas diferentes (server2 e server4, respectivamente), para garantir que os servi\u00e7os n\u00e3o competissem por recursos.</p> <pre><code>juju deploy ./prometheus2_XXX.charm\n</code></pre> <pre><code>juju deploy ./grafana_XXX.charm\n</code></pre> <p>OBS: XXX \u00e9 a vers\u00e3o do charm, que no caso foi r69</p> <p>Apesar disso, tivemos um problema com o deploy do Grafana, pois a vers\u00e3o do charm n\u00e3o enxergava a imagem do ubuntu 22.04, que era a vers\u00e3o utilizada nas m\u00e1quinas. Para resolver, o pr\u00f3prio Juju instalou a vers\u00e3o 20.04. No entanto, para concluir o processo, foi necess\u00e1rio utilizar a flag <code>--force</code> no comando de deploy para garantir que o processo fosse conclu\u00eddo.</p> <pre><code>juju deploy ./grafana_r69.charm --force\n</code></pre>"},{"location":"roteiro2/main/#3-integracao-entre-grafana-e-prometheus","title":"3. Integra\u00e7\u00e3o entre Grafana e Prometheus","text":"<p>No Juju, uma integra\u00e7\u00e3o \u00e9 uma conex\u00e3o entre duas ou mais aplica\u00e7\u00f5es. Essa conex\u00e3o \u00e9 estabelecida devido ao fato de uma aplica\u00e7\u00e3o ter um endpoint (ponto de comunica\u00e7\u00e3o) espec\u00edfico que permite a intera\u00e7\u00e3o com outras aplica\u00e7\u00f5es.</p> <p>Grafana pode se conectar ao Prometheus atrav\u00e9s de uma rela\u00e7\u00e3o, onde o Prometheus exp\u00f5e m\u00e9tricas e o Grafana as consome para exibir visualmente.</p> <p>Para isso, \u00e9 necess\u00e1rio configurar o Prometheus como um data source no Grafana.</p> <pre><code>juju integrate prometheus2:grafana\u2013source grafana:grafana-source \n</code></pre>"},{"location":"roteiro2/main/#4-acessando-o-grafana","title":"4. Acessando o Grafana","text":"<p>Para acessar o Grafana rodando no server4 por meio de um t\u00fanel SSH. Criamos uma conex\u00e3o segura entre o nosso localhost e o server4, utilizando a funcionalidade de redirecionamento de portas do SSH. </p> <p>Esse t\u00fanel permite que o tr\u00e1fego da porta 8001 no nosso computador local seja redirecionado para a porta 3000 no server4, onde o Grafana est\u00e1 ouvindo, como se estivesse rodando localmente em nossa m\u00e1quina.</p> <pre><code>ssh cloud@10.103.1.19 -L 8001:172.16.0.25:3000\n</code></pre> <p>Assim, ao acessar o endere\u00e7o <code>http://localhost:8001</code> no navegador, conseguimos visualizar o Grafana.</p> <p>OBS: Para conseguir acessar o Grafana, foi necess\u00e1rio pegar a senha gerada pelo Juju e inserir no login.</p> <pre><code>juju run grafana/1 get-admin-password\n</code></pre> <p>Por fim, criamos um novo dashboard no Grafana, onde adicionamos o Prometheus como data source.</p>"},{"location":"roteiro2/main/#tarefa-1","title":"Tarefa 1","text":"<p>Verificando se a integra\u00e7\u00e3o foi feita corretamente</p> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas</p> <p></p> <p>Estado atual de todos os deployments no Juju</p> <p></p> <p>Tela do Dashboard do Grafana com o Prometheus como source</p> <p></p> <p>Acesso remoto ao Dashboard a partir da rede do Insper</p> <p></p> <p>Aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU</p>"},{"location":"roteiro2/main/#discussao","title":"Discuss\u00e3o","text":"<ol> <li> <p>A combina\u00e7\u00e3o entre Juju e MAAS simplificou a automa\u00e7\u00e3o do gerenciamento de infraestrutura, permitindo o provisionamento r\u00e1pido e eficiente de servidores f\u00edsicos e virtuais.</p> </li> <li> <p>O uso de charms para o deployment de servi\u00e7os garantiu uma instala\u00e7\u00e3o padronizada e a consist\u00eancia entre diferentes ambientes, o que facilita a replica\u00e7\u00e3o e a escalabilidade da infraestrutura.</p> </li> <li> <p>A utiliza\u00e7\u00e3o de tags no MAAS e modelos no Juju foi crucial para separar e organizar ambientes de forma eficiente, promovendo uma gest\u00e3o mais clara e estruturada da infraestrutura.</p> </li> <li> <p>A incompatibilidade de vers\u00e3o do charm Grafana com a vers\u00e3o do Ubuntu ataraplhou o desenvolvimento foi resolvida utilizando a flag --force, permitindo a continuidade do deployment sem maiores impactos.</p> </li> <li> <p>A integra\u00e7\u00e3o entre Grafana e Prometheus proporcionou uma vis\u00e3o clara e em tempo real do desempenho da infraestrutura, facilitando a an\u00e1lise e a resolu\u00e7\u00e3o de poss\u00edveis problemas.</p> </li> <li> <p>O SSH foi utilizado para acessar de forma segura os dashboards do Grafana, garantindo que a comunica\u00e7\u00e3o com a infraestrutura fosse protegida.</p> </li> </ol>"},{"location":"roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Em suma, a integra\u00e7\u00e3o entre Juju e MAAS, junto ao uso de charms para deployment, foi fundamental para automatizar e organizar de forma eficaz o gerenciamento da infraestrutura. A compatibilidade e os problemas t\u00e9cnicos foram superados com rapidez, garantindo uma implementa\u00e7\u00e3o bem-sucedida. O uso de ferramentas como Grafana e Prometheus permitiu monitorar em tempo real a performance, proporcionando visibilidade e controle essenciais para a otimiza\u00e7\u00e3o cont\u00ednua dos servi\u00e7os. A automa\u00e7\u00e3o oferecida pelo Juju se mostrou uma solu\u00e7\u00e3o poderosa, facilitando o gerenciamento e manuten\u00e7\u00e3o da infraestrutura.</p>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":""},{"location":"roteiro3/main/#objetivo","title":"Objetivo","text":"<p>Criar uma Private Cloud com OpenStack</p> <p>Para esse projeto, vamos criar uma Cloud privada. Para isso vamos instalar v\u00e1rios softwares para deixar a cloud funcional.</p> <p>Os softwares ser\u00e3o instalados em m\u00e1quinas, sempre buscando distribuir as cargas entre as m\u00e1quinas. Para isso, utilizaremos o Juju, que \u00e9 um software de orquestra\u00e7\u00e3o de containers. O Juju \u00e9 um software que permite gerenciar aplica\u00e7\u00f5es em nuvem, como OpenStack, Kubernetes, etc.</p>"},{"location":"roteiro3/main/#infraestrutura","title":"Infraestrutura","text":"<p>Para iniciar o roteiro 3, precisamos criar novamente as bridges, dentro do maas, para que as m\u00e1quinas virtuais possam receber IPs.</p> <p>Como no roteiro passado, vamos criar um controller na m\u00e1quina 1, para isso criamos uma tag <code>juju</code> que referencia a m\u00e1quina 1.</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Definimos o modelo openstack que vai ser utilizado para fazer o deploy. <pre><code>juju add-model --config default-series=jammy openstack\n</code></pre> <pre><code>juju switch maas-controller:openstack\n</code></pre></p> <p>Criamos o dashboard do juju dentro de uma m\u00e1quina virtual em cima da m\u00e1quina 0, para isso utilizamos o controller criado anteriormente. Foram tamb\u00e9m integrados o juju-dashboard com o controller e exposto para a rede.</p> <pre><code>juju deploy juju-dashboard --to lxd:0\n</code></pre> <p>OBS: Lembrando que o lxd \u00e9 um docker que tem ubuntu ativo, serve para criar uma maquina virtual em cima de uma m\u00e1quina f\u00edsica.</p> <pre><code>juju integrate juju-dashboard controller\n</code></pre> <pre><code>juju expose juju-dashboard\n</code></pre> <p>Instala\u00e7\u00e3o do Ceph OSD que funciona como um software que unificar\u00e1 todas os HDs das m\u00e1quinas para criar um storage distribu\u00eddo. Esse software \u00e9 justamente o que vai permitir que os usu\u00e1rios consigam ver apenas uma unidade de armazenamento, mesmo que na verdade sejam v\u00e1rias m\u00e1quinas com v\u00e1rios HDs. Para isso, utilizamos o charm do ceph-osd.</p> <p>Para isso criamos tags espec\u00edficas para tr\u00eas m\u00e1quinas que v\u00e3o ser utilizadas para o ceph, no caso as m\u00e1quinas 3, 4 e 5. Essas tags s\u00e3o compute.</p> <pre><code>juju deploy -n 3 --channel quincy/stable --config ceph-osd.yaml --constraints tags=compute ceph-osd\n</code></pre> <p>Quando o workload do ceph estiver blocked e o agent estiver idle, significa que ele esta esperando o deploy de um novo software.</p> <p>Nova compute funciona para prover instancias de novas unidades de armazenamento. Fizemos a altera\u00e7\u00e3o do arquivo <code>nova-compute.yaml</code> com a seguinte configura\u00e7\u00e3o: <pre><code>nova-compute:\n  config-flags: default_ephemeral_format=ext4\n  enable-live-migration: true\n  enable-resize: true\n  migration-auth-type: ssh\n  virt-type: qemu\n</code></pre></p> <pre><code>juju deploy -n 3 --to 0,1,2 --channel yoga/stable --config nova-compute.yaml nova-compute\n</code></pre> <p>Para cria\u00e7\u00e3o do mysql, utilizamos o charm do mysql, que \u00e9 um banco de dados relacional. O charm do mysql \u00e9 utilizado para armazenar os dados do openstack. Para ele funcionar, precisamos tamb\u00e9m de 3 m\u00e1quinas</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel 8.0/stable mysql-innodb-cluster\n</code></pre> <p>Ap\u00f3s isso, precisamos criar o banco de dados do openstack, para isso utilizamos o charm do mysql-router. O mysql-router \u00e9 um software que faz a comunica\u00e7\u00e3o entre o banco de dados e o openstack. Fizemos a instala\u00e7\u00e3o do Vault, que \u00e9 um software de gerenciamento de senhas. O Vault \u00e9 um reposit\u00f3rio de chaves que \u00e9 utilizado para criptografar a comunica\u00e7\u00e3o entre aplica\u00e7\u00f5es cloud, ou seja, ele guarda as chaves para todas as comunica\u00e7\u00f5es entre aplica\u00e7\u00f5es do openstack.</p> <pre><code>juju deploy --to lxd:2 vault --channel 1.8/stable\n</code></pre> <pre><code>sudo snap install vault\n</code></pre> <p>Inicializamos o vault, para isso precisamos de um token de inicializa\u00e7\u00e3o. O token \u00e9 gerado pelo vault e \u00e9 utilizado para autenticar o usu\u00e1rio. O token \u00e9 gerado apenas uma vez e deve ser guardado em um local seguro. Para isso, utilizamos o comando <code>juju run-action</code> que executa um comando em uma m\u00e1quina espec\u00edfica.</p> <pre><code>export VAULT_ADDR=\"http://172.16.0.30:8200\"\nvault operator init -key-shares=5 -key-threshold=3\n</code></pre> <p><pre><code>juju run vault/leader authorize-charm token=XXX\n</code></pre> OBS: O XXX \u00e9 o token gerado pelo vault, como exemplo s.QMhaOED3UGQ4MeH3fmGOpNED</p> <p>Fizemos a implementa\u00e7\u00e3o da Neutron networking, que fornece a conex\u00e3o de rede no OpenStack. Ela \u00e9 respons\u00e1vel pelo gerenciamento da rede para as m\u00e1quinas virtuais e outros recursos. Inicialmente, criamos pontes para as m\u00e1quinas (br-ex) pelo maas A flat-network-providers habilita uma rede sem VLANs. Isso \u00e9 essencial para acessar redes externas, como a Internet. A ovn-bridge-mappings liga a bridge OVS (br-ex) ao provedor de rede</p> <p>OVN aplica\u00e7\u00e3o \u00e9 ovn-central e requer 3 m\u00e1quinas</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel 22.03/stable ovn-central\n</code></pre> <p>Neutron-api em cont\u00eainer LXD no n\u00f3 1</p> <pre><code>juju deploy --to lxd:1 --channel yoga/stable --config neutron.yaml neutron-api\n</code></pre> <p>Ap\u00f3s isso, foi feito o deploy do charm das aplica\u00e7\u00f5es com o neutron-api-plugin-ovn e ovn-chassis charm</p> <pre><code>juju deploy --channel yoga/stable neutron-api-plugin-ovn\njuju deploy --channel 22.03/stable --config neutron.yaml ovn-chassis\n</code></pre> <p>E suas respectivas rela\u00e7\u00f5es que servem para conectar os softwares, ou seja, para que eles consigam se comunicar entre si.</p> <pre><code>juju integrate neutron-api-plugin-ovn:neutron-plugin neutron-api:neutron-plugin-api-subordinate\njuju integrate neutron-api-plugin-ovn:ovsdb-cms ovn-central:ovsdb-cms\njuju integrate ovn-chassis:ovsdb ovn-central:ovsdb\njuju integrate ovn-chassis:nova-compute nova-compute:neutron-plugin\njuju integrate neutron-api:certificates vault:certificates\njuju integrate neutron-api-plugin-ovn:certificates vault:certificates\njuju integrate ovn-central:certificates vault:certificates\njuju integrate ovn-chassis:certificates vault:certificates\n</code></pre> <p>Por fim conectamos o neutron-api com a base de dados do OpenStack</p> <pre><code>juju deploy --channel 8.0/stable mysql-router neutron-api-mysql-router\njuju integrate neutron-api-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate neutron-api-mysql-router:shared-db neutron-api:shared-db\n</code></pre> <p>O Keystone \u00e9 o servi\u00e7o de identidade do OpenStack. Ele \u00e9 respons\u00e1vel por autenticar usu\u00e1rios, fornecer tokens, e fazer o controle de acesso, para isso ele faz a troca do par de chaves.</p> <pre><code>juju deploy --to lxd:0 --channel yoga/stable keystone\n</code></pre> <p>Interligamos o keystone com a base de dados</p> <pre><code>juju deploy --channel 8.0/stable mysql-router keystone-mysql-router\njuju integrate keystone-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate keystone-mysql-router:shared-db keystone:shared-db\n</code></pre> <pre><code>juju integrate keystone:identity-service neutron-api:identity-service\njuju integrate keystone:certificates vault:certificates\n</code></pre> <p>O RabbitMQ \u00e9 o servi\u00e7o de mensageria do OpenStack. Ele \u00e9 respons\u00e1vel por enviar mensagens entre os servi\u00e7os do OpenStack</p> <pre><code>juju deploy --to lxd:2 --channel 3.9/stable rabbitmq-server\n</code></pre> <pre><code>juju integrate rabbitmq-server:amqp neutron-api:amqp\njuju integrate rabbitmq-server:amqp nova-compute:amqp\n</code></pre> <p>Nova Cloud Controller coordena as a\u00e7\u00f5es de provisionamento de m\u00e1quinas virtuais. Ele \u00e9 respons\u00e1vel por criar, excluir e gerenciar inst\u00e2ncias de m\u00e1quinas virtuais.</p> <p>Para isso, definimos que o gerenciador de rede \u00e9 o Neutron</p> <pre><code>nova-cloud-controller:\n  network-manager: Neutron\n</code></pre> <pre><code>juju deploy --to lxd:2 --channel yoga/stable --config ncc.yaml nova-cloud-controller\n</code></pre> <p>Interligando o nova-cloud-controller com a base de dados</p> <pre><code>juju deploy --channel 8.0/stable mysql-router ncc-mysql-router\njuju integrate ncc-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate ncc-mysql-router:shared-db nova-cloud-controller:shared-db\n</code></pre> <p>OBS: nova-cloud-controller-mysql-router = ncc-mysql-router</p> <pre><code>juju integrate nova-cloud-controller:identity-service keystone:identity-service\njuju integrate nova-cloud-controller:amqp rabbitmq-server:amqp\njuju integrate nova-cloud-controller:neutron-api neutron-api:neutron-api\njuju integrate nova-cloud-controller:cloud-compute nova-compute:cloud-compute\njuju integrate nova-cloud-controller:certificates vault:certificates\n</code></pre> <p>O servi\u00e7o Placement do OpenStack \u00e9 respons\u00e1vel por rastrear os recursos f\u00edsicos dispon\u00edveis, como CPU, mem\u00f3ria e armazenamento. Ele fornece informa\u00e7\u00f5es sobre a disponibilidade de recursos para os servi\u00e7os do OpenStack</p> <pre><code>juju deploy --to lxd:0 --channel yoga/stable placement\n</code></pre> <p>Interligando o placement com a base de dados</p> <pre><code>juju deploy --channel 8.0/stable mysql-router placement-mysql-router\njuju integrate placement-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate placement-mysql-router:shared-db placement:shared-db\n</code></pre> <pre><code>juju integrate placement:identity-service keystone:identity-service\njuju integrate placement:placement nova-cloud-controller:placement\njuju integrate placement:certificates vault:certificates\n</code></pre> <p>Horizon tamb\u00e9m foi instalado. Ela \u00e9 a interface web usada para gerenciar os servi\u00e7os do OpenStack de forma visual. </p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable openstack-dashboard\n</code></pre> <pre><code>juju deploy --channel 8.0/stable mysql-router dashboard-mysql-router\njuju integrate dashboard-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate dashboard-mysql-router:shared-db openstack-dashboard:shared-db\n</code></pre> <pre><code>juju integrate openstack-dashboard:identity-service keystone:identity-service\njuju integrate openstack-dashboard:certificates vault:certificates\n</code></pre> <p>O Glance \u00e9 o servi\u00e7o de gerenciamento de imagens de disco no OpenStack. Ele permite que usu\u00e1rios armazenem, descubram e recuperem imagens de m\u00e1quinas virtuais.</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable glance\n</code></pre> <p>Interligando o glance com a base de dados</p> <pre><code>juju deploy --channel 8.0/stable mysql-router glance-mysql-router\njuju integrate glance-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate glance-mysql-router:shared-db glance:shared-db\n</code></pre> <pre><code>juju integrate glance:image-service nova-cloud-controller:image-service\njuju integrate glance:image-service nova-compute:image-service\njuju integrate glance:identity-service keystone:identity-service\njuju integrate glance:certificates vault:certificates\n</code></pre> <p>O Ceph Monitor \u00e9 o servi\u00e7o respons\u00e1vel por manter a vis\u00e3o consistente e distribu\u00edda do cluster de armazenamento Ceph. Ele rastreia o estado dos OSDs (Object Storage Daemons) e ajuda a coordenar as opera\u00e7\u00f5es entre os n\u00f3s de armazenamento, al\u00e9m de monitorar a sa\u00fade do cluster.</p> <pre><code>ceph-mon:\n  expected-osd-count: 3\n  monitor-count: 3\n</code></pre> <p>OBS: Composto por tr\u00eas n\u00f3s e que deve esperar pelo menos tr\u00eas OSDs</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel quincy/stable --config ceph-mon.yaml ceph-mon\n</code></pre> <pre><code>juju integrate ceph-mon:osd ceph-osd:mon\njuju integrate ceph-mon:client nova-compute:ceph\njuju integrate ceph-mon:client glance:ceph\n</code></pre> <p>O Cinder \u00e9 o servi\u00e7o de armazenamento em bloco do OpenStack. Ele fornece volumes de armazenamento que podem ser anexados a inst\u00e2ncias de m\u00e1quinas virtuais. Essa etapa consiste em configurar o servi\u00e7o principal e integr\u00e1-lo com Ceph como backend.</p> <p>A configura\u00e7\u00e3o do cinder \u00e9 feita atrav\u00e9s de um arquivo YAML, onde definimos o backend de armazenamento e a vers\u00e3o da API do Glance.</p> <pre><code>cinder:\n  block-device: None\n  glance-api-version: 2\n</code></pre> <pre><code>juju deploy --to lxd:1 --channel yoga/stable --config cinder.yaml cinder\n</code></pre> <pre><code>juju deploy --channel 8.0/stable mysql-router cinder-mysql-router\njuju integrate cinder-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate cinder-mysql-router:shared-db cinder:shared-db\n</code></pre> <p>Essas pr\u00f3ximas rela\u00e7\u00f5es conectam o Cinder com os servi\u00e7os principais do OpenStack: identidade (Keystone), imagens (Glance), mensageria (RabbitMQ), orquestra\u00e7\u00e3o de volumes (Nova), e certificados (Vault)</p> <pre><code>juju integrate cinder:cinder-volume-service nova-cloud-controller:cinder-volume-service\njuju integrate cinder:identity-service keystone:identity-service\njuju integrate cinder:amqp rabbitmq-server:amqp\njuju integrate cinder:image-service glance:image-service\njuju integrate cinder:certificates vault:certificates\n</code></pre> <p>Para que o Cinder use o Ceph como backend:</p> <pre><code>juju deploy --channel yoga/stable cinder-ceph\n</code></pre> <pre><code>juju integrate cinder-ceph:storage-backend cinder:storage-backend\njuju integrate cinder-ceph:ceph ceph-mon:client\njuju integrate cinder-ceph:ceph-access nova-compute:ceph-access\n</code></pre> <p>O Ceph RADOS Gateway (RGW) \u00e9 um servi\u00e7o de armazenamento de objetos que fornece uma interface compat\u00edvel com S3 e Swift. Ele funciona como alternativa ao servi\u00e7o OpenStack Swift ou Amazon S3.</p> <pre><code>juju deploy --to lxd:0 --channel quincy/stable ceph-radosgw\n</code></pre> <p>Integrando o Ceph RADOS Gateway com o cluster do Ceph. Essa rela\u00e7\u00e3o conecta o gateway aos monitores do Ceph, permitindo o gerenciamento de pools e autentica\u00e7\u00e3o de usu\u00e1rios S3/Swift</p> <pre><code>juju integrate ceph-radosgw:mon ceph-mon:radosgw\n</code></pre> <p>Por fim, como a aplica\u00e7\u00e3o do Ceph-OSD j\u00e1 foi realizada, precisamos fazer a integra\u00e7\u00e3o dele para finalizar a etapa de configura\u00e7\u00e3o da infraestrutura. A rela\u00e7\u00e3o proposta vai funcionar para configura\u00e7\u00e3o do charm para uso do disco <code>/dev/sdb</code>, que vai ser utilizado para armazenamento.</p> <pre><code>juju config ceph-osd osd-devices='/dev/sdb'\n</code></pre>"},{"location":"roteiro3/main/#setup","title":"Setup","text":"<p>Iremos agora configurar os servi\u00e7os que controlam as Virtual Machines, o Volume de Disco e a Estrutura de Rede Virtual Primeiro, vamos carregar as vari\u00e1veis de ambiente e se autenticar no OpenStack</p> <p>Para isso, utilizamos o comando <code>openrc</code> que \u00e9 um script que carrega as vari\u00e1veis de ambiente necess\u00e1rias para autentica\u00e7\u00e3o. O script \u00e9 gerado pelo OpenStack e deve ser baixado na interface web do OpenStack.</p> <pre><code>sudo snap install openstackclients\n</code></pre> <p>Baixamos o arquivo direto da interface web do OpenStack e configuramos o ambiente do usu\u00e1rio administrador</p> <pre><code>wget https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/_downloads/c894c4911b9572f0b5f86bdfc5d12d8e/openrc\n</code></pre> <pre><code>source ~/openrc\n</code></pre> <pre><code>env | grep OS_\n</code></pre> <p>Para entrada no OpenStack dashboard fizemos um t\u00fanel SSH para acessar o dashboard do OpenStack.</p> <pre><code>ssh cloud@10.103.0.X -L 8001:172.16.0.39:80\n</code></pre> <p>O acesso ao dashboard \u00e9 feito pelo navegador, acessando o endere\u00e7o <code>http://localhost:8001</code> e utilizando o usu\u00e1rio admin e a senha obtido pelo <code>env | grep OS_</code></p>"},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa 1","text":"<p>Verificando se o acesso ao Dashboard do OpenStack est\u00e1 funcionando, ou seja, se conseguimos acessar a interface web do OpenStack.</p> <p></p> <p>Status do JUJU</p> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas</p> <p></p> <p>Aba compute overview no OpenStack Dashboard</p> <p></p> <p>Aba compute instances no OpenStack Dashboard</p> <p></p> <p>Aba network topology no OpenStack Dashboard</p> <p>Criamos o cliente via snap, carregamos o openrc e verificamos se o cliente est\u00e1 funcionando</p> <pre><code>source openrc\n</code></pre> <pre><code>openstack service list\n</code></pre> <p>Al\u00e9m disso, fizemos pequenos ajustes de rede <pre><code>juju config neutron-api enable-ml2-dns=\"true\"\njuju config neutron-api-plugin-ovn dns-servers=\"172.16.0.1\"\n</code></pre></p> <p>Carregamos a imagem do Ubuntu 22.04 no OpenStack, para isso utilizamos o comando <code>openstack image create</code> que cria uma imagem no OpenStack. O comando \u00e9 utilizado para criar uma imagem de disco que pode ser utilizada para criar inst\u00e2ncias de m\u00e1quinas virtuais.</p> <pre><code>mkdir ~/cloud-images\n\nwget http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \\\n   -O ~/cloud-images/jammy-amd64.img\n</code></pre> <pre><code>openstack image create --public --container-format bare \\\n   --disk-format qcow2 --file ~/cloud-images/jammy-amd64.img \\\n   jammy-amd64\n</code></pre> <p>Para definir um perfil de hardware para a imagem, utilizamos o comando <code>openstack flavor create</code> que cria um perfil de hardware no OpenStack. O comando pode ser utilizado para criar inst\u00e2ncias de m\u00e1quinas virtuais.</p> <pre><code>openstack flavor create --ram 1024 --disk 20 --vcpus 1 m1.tiny\n</code></pre> <pre><code>openstack flavor create --ram 2048 --disk 20 --vcpus 1 m1.small\n</code></pre> <pre><code>openstack flavor create --ram 4096 --disk 20 --vcpus 2 m1.medium\n</code></pre> <pre><code>openstack flavor create --ram 8192 --disk 20 --vcpus 4 m1.large\n</code></pre> <p>OBS: O par\u00e2metro <code>--ram</code> define a quantidade de mem\u00f3ria RAM da inst\u00e2ncia e o par\u00e2metro <code>--disk</code> define o tamanho do disco da inst\u00e2ncia. Todos esses foram configurados de acordo com a tabela abaixo:</p> Nome do perfil vCPUs Ram (GB) Disk m1.tiny 1 1 20 m1.small 1 2 20 m1.medium 2 4 20 m1.large 4 8 20 <p>Criando a rede externa, que \u00e9 a rede que vai permitir o acesso \u00e0 internet. Para isso, utilizamos o comando <code>openstack network create</code> que cria uma rede no OpenStack. O comando \u00e9 utilizado para criar uma rede externa que pode ser utilizada para conectar as inst\u00e2ncias de m\u00e1quinas virtuais \u00e0 internet.</p> <pre><code>openstack network create --external --share- \\\n   --provider-network-type flat -provider-physical-network physnet1 \\\n    ext_net\n</code></pre> <p>E uma sub-rede externa, que \u00e9 a sub-rede que vai permitir o acesso com o comando <code>openstack subnet create</code> que cria uma sub-rede no OpenStack. O comando \u00e9 utilizado para criar uma sub-rede externa que pode ser utilizada para conectar as inst\u00e2ncias de m\u00e1quinas virtuais \u00e0 internet.</p> <pre><code>openstack subnet create --network ext_net --no-dhcp \\\n   --gateway 172.16.0.1 --subnet-range 172.16.0.0/20 \\\n   --allocation-pool start=172.16.7.0,end=172.16.8.255 \\\n   ext_subnet\n</code></pre> <p>Criando uma rede interna, que \u00e9 a rede que vai permitir a comunica\u00e7\u00e3o entre as inst\u00e2ncias de m\u00e1quinas virtuais. Para isso, utilizamos o comando <code>openstack network create</code> que cria uma rede no OpenStack. O comando \u00e9 utilizado para criar uma rede interna que pode ser utilizada para conectar as inst\u00e2ncias de m\u00e1quinas virtuais.</p> <pre><code>openstack network create --internal user1_net\n</code></pre> <p>E uma sub-rede interna, que \u00e9 a sub-rede que vai permitir a comunica\u00e7\u00e3o entre as inst\u00e2ncias de m\u00e1quinas virtuais com o comando <code>openstack subnet create</code> que cria uma sub-rede no OpenStack. O comando \u00e9 utilizado para criar uma sub-rede interna que pode ser utilizada para conectar as inst\u00e2ncias de m\u00e1quinas virtuais.</p> <pre><code>openstack subnet create --network user1_net  \\\n   --subnet-range 192.169.0.0/24 \\\n   --allocation-pool start=192.169.0.10,end=192.169.0.100 \\\n   user1_subnet\n</code></pre> <p>Por fim adicionamos a rota do user1 a subnet criada</p> <pre><code>openstack router create user1_router\nopenstack router add subnet user1_router user1_subnet\nopenstack router set user1_router --external-gateway ext_net\n</code></pre> <p>Para verificar se est\u00e1 tudo funcionando, criamos uma inst\u00e2ncia que como teste pr\u00e1tico do ambiente completo que configuramos. Em outras palavras, ela \u00e9 o primeiro \"cliente\" real da nuvem que montamos.</p> <pre><code>openstack server create --image jammy-amd64 --flavor m1.tiny \\\n   --key-name user1 --network user1_net --security-group Allow_SSH \\\n   client\n</code></pre> <p>Al\u00e9m disso, alocamos um IP flutuante para a inst\u00e2ncia criada, que \u00e9 o IP que vai ser utilizado para acessar a inst\u00e2ncia de fora da nuvem. Agora teremos um IP externo e um IP interno.</p> <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip client $FLOATING_IP\n</code></pre>"},{"location":"roteiro3/main/#tarefa-2","title":"Tarefa 2","text":"<p>Verificando se o acesso ao Dashboard do OpenStack est\u00e1 funcionando, ou seja, se conseguimos acessar a interface web do OpenStack.</p> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas</p> <p></p> <p>Aba compute overview no OpenStack</p> <p></p> <p>Aba compute instances no OpenStack</p> <p></p> <p>Aba network topology no OpenStack</p>"},{"location":"roteiro3/main/#diferencas-encontradas-entre-os-prints-das-telas-na-tarefa-1-e-na-tarefa-2","title":"Diferen\u00e7as encontradas entre os prints das telas na Tarefa 1 e na Tarefa 2","text":"<p>Inst\u00e2ncias:</p> <p>Na Tarefa 1, o painel mostra nenhuma inst\u00e2ncia criada, pois o ambiente ainda n\u00e3o foi completamente configurado.</p> <p>Na Tarefa 2, j\u00e1 existe uma inst\u00e2ncia do tipo m1.tiny chamada client, indicando que os recursos de computa\u00e7\u00e3o foram corretamente configurados e utilizados.</p> <p>Topologia de Rede:</p> <p>Na Tarefa 1, a topologia est\u00e1 vazia, com aus\u00eancia de redes, roteador e associa\u00e7\u00f5es entre eles.</p> <p>Na Tarefa 2, a topologia exibe demonstra que toda a estrutura l\u00f3gica de rede est\u00e1 funcional visto que:</p> <ul> <li> <p>A rede ext_net (azul) com faixa 172.16.0.0/24 representa a rede externa, usada para acesso p\u00fablico via Floating IP.</p> </li> <li> <p>A rede user1_net (laranja) com faixa 192.168.0.0/24 \u00e9 a rede interna, onde ficam as inst\u00e2ncias.</p> </li> <li> <p>Um roteador foi criado conectando as duas redes, permitindo roteamento entre inst\u00e2ncias privadas e a rede externa.</p> </li> </ul> <p>Overview:</p> <p>Na Tarefa 1, os gr\u00e1ficos e m\u00e9tricas indicam zero uso de vCPUs, RAM e disco, refletindo a aus\u00eancia de workloads ativos.</p> <p>Na Tarefa 2, essas m\u00e9tricas apresentam uso compat\u00edvel com a inst\u00e2ncia m1.tiny ativa, evidenciando o provisionamento correto dos recursos de infraestrutura</p>"},{"location":"roteiro3/main/#como-os-recursos-foram-criados","title":"Como os recursos foram criados","text":"<ul> <li>Inst\u00e2ncia client:</li> </ul> <p>A inst\u00e2ncia client foi criada utilizando, especificando a imagem, flavor e key pair, utilizamos tamb\u00e9m o m1.tiny que \u00e9 o mais leve pois n\u00e3o \u00e9 necess\u00e1rio muito recurso. Por fim foi alocado um IP flutuante para acesso externo.</p> <ul> <li>Rede e sub-rede:</li> </ul> <p>Foram criadas duas redes no OpenStack: ext_net (externa, faixa 172.16.0.0/24) e user1_net (interna, faixa 192.168.0.0/24).</p> <ul> <li>Conex\u00e3o:</li> </ul> <p>A conex\u00e3o entre as redes foi feita atrav\u00e9s de um roteador, que conecta a rede interna \u00e0 externa, permitindo comunica\u00e7\u00e3o entre inst\u00e2ncias e acesso \u00e0 internet.</p> <p>Com a m\u00e1quina nova, que estava reservada, iremos adicionar um novo n\u00f3 de computa\u00e7\u00e3o. O objetivo \u00e9 preparar o ambiente para alta disponibilidade, maior desempenho e capacidade.</p> <pre><code>juju add-unit nova-compute\n</code></pre> <p>Al\u00e9m disso, convertemos o mesmo n\u00f3 tamb\u00e9m em um n\u00f3 de armazenamento .</p> <p><pre><code>juju add-unit --to &lt;machine-id&gt; ceph-osd\n</code></pre> Usamos a maquina 3, que j\u00e1 estava reservada para isso. </p>"},{"location":"roteiro3/main/#tarefa-3","title":"Tarefa 3","text":"<p>Nossa estrutura de rede nesse momento est\u00e1 assim:</p> <p></p> <p>Arquitetura de rede da infraestrutura</p>"},{"location":"roteiro3/main/#uso-da-infraestrutura","title":"Uso da Infraestrutura","text":"<p>Para utilizar a infraestrutura criada, precisamos criar uma m\u00e1quina virtual para cada tarefa especificada. Foi solicitado 2 inst\u00e2ncias com a API do projeto, etapa 1; 1 inst\u00e2ncia com banco de dados, etapa 1, e; 1 inst\u00e2ncia com LoadBalancer, Nginx.</p> <p>Para isso, fizemos novamente IPs flutuantes para cada inst\u00e2ncia criada, e utilizamos o mesmo processo de cria\u00e7\u00e3o de inst\u00e2ncias que foi utilizado anteriormente. Apenas nesse in\u00edcio iremos continuar com todos os IPs externos, para podermos verificar se tudo est\u00e1 funcionando corretamente. Ap\u00f3s os testes e instala\u00e7\u00f5es dos softwares, iremos retirar os IPs externos e deixar apenas no load balancer, para que possamos acessar o servi\u00e7o de forma externa.</p> <p>Primeiro instalamos o docker em cada uma das m\u00e1quinas virtuais que iriam receber as APIs, para isso utilizamos os seguintes comandos:</p> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Ap\u00f3s isso, baixamos e rodamos a imagem do docker hub</p> <pre><code>sudo docker pull vitorpadova/projeto_cloud\n</code></pre> <pre><code>docker run -d -p 8080:80 vitorpadova/projeto_cloud\n</code></pre> <p>Para ver se a imagem est\u00e1 rodando, utilizamos o comando:</p> <pre><code>sudo docker ps -a\n</code></pre> <p>Fizemos o mesmo processo para a m\u00e1quina que vai receber o banco de dados, mas nesse caso utilizamos a imagem do Postgres.</p> <p>Depois de instalar o docker, baixamos a imagem do Postgres e rodamos o container com o seguinte comando:</p> <pre><code>sudo docker run --env-file .env vitorpadova/projeto_cloud\n</code></pre> <p>O mesmo processo foi feito para o LoadBalancer, mas nesse caso utilizamos a imagem do Nginx.</p> <p>Depois de tudo, ligamos o LoadBalancer com as APIs e o Banco de Dados, para isso criamos um arquivo de configura\u00e7\u00e3o env com as vari\u00e1veis de ambiente necess\u00e1rias para o funcionamento.</p> <p>Por fim, removemos os IPs flutuantes das inst\u00e2ncias, exceto do LoadBalancer, para que possamos acessar o servi\u00e7o de forma externa.</p> <pre><code>openstack server remove floating ip client $FLOATING_IP\n</code></pre>"},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":"<p>Verificando se o app est\u00e1 funcionando.</p> <p></p> <p>Arquitetura de rede da infraestrutura</p> <p></p> <p>Lista de VMs utilizadas com nome e IPs alocados</p> <p></p> <p>Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB</p> <p></p> <p>Server (m\u00e1quina f\u00edsica) que Load Balancer foi alocado pelo OpenStack</p> <p></p> <p>Server (m\u00e1quina f\u00edsica) que Base de Dados foi alocada pelo OpenStack</p> <p></p> <p>Server (m\u00e1quina f\u00edsica) que API 1 foi alocada pelo OpenStack</p> <p></p> <p>Server (m\u00e1quina f\u00edsica) que API 2 foi alocada pelo OpenStack</p>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#objetivo","title":"Objetivo","text":"<ol> <li> <p>Compreender os fundamentos de Infraestrutura como C\u00f3digo (IaC) e sua import\u00e2ncia em ambientes de nuvem.</p> </li> <li> <p>Entender os conceitos b\u00e1sicos de SLA (Service Level Agreement) e DR (Disaster Recovery) e como eles se relacionam</p> </li> <li> <p>Aplicar os conceitos aprendidos em uma pr\u00e1tica de cria\u00e7\u00e3o de infraestrutura automatizada, organizada em hierarquias de projeto e aplica\u00e7\u00f5es isoladas, ou seja, criar uma infraestrutura em cloud com esse script</p> </li> </ol>"},{"location":"roteiro4/main/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>Infraestrutura como C\u00f3digo (IaC) \u00e9 uma abordagem que permite gerenciar e provisionar recursos de infraestrutura por meio de c\u00f3digo, em vez de processos manuais. Isso traz agilidade, consist\u00eancia e escalabilidade para ambientes de nuvem. Portanto para processos de subir uma m\u00e1quina virtual, criar um banco de dados ou configurar uma rede, o IaC permite que esses processos sejam realizados de forma automatizada e repet\u00edvel.</p> <p>Terraform \u00e9 uma ferramenta de IaC que permite criar uma infraestrutura em cloud. Para isso, ele precisa ter as chaves e senhas para poder criar os recursos. Portanto ele disponibiliza arquivos scripts que s\u00e3o utilizados para criar a infraestrutura desejada. Esses arquivos s\u00e3o escritos em HCL (HashiCorp Configuration Language), uma linguagem declarativa que permite descrever a infraestrutura de forma simples.</p> <p>Instalando o Terraform</p> <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\n\ngpg --no-default-keyring --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg --fingerprint\n\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre>"},{"location":"roteiro4/main/#infraestrutura","title":"Infraestrutura","text":"<p>Agora que voc\u00ea j\u00e1 tem o Terraform instalado, vamos criar uma infraestrutura em cloud. Para isso, vamos criar utilizando c\u00f3digos.</p> <p>Ser\u00e1 necess\u00e1rio criar um Dom\u00ednio, dois projetos e um usu\u00e1rio \"Aluno\", tudo isso via dashboard do OpenStack, Horizon.</p> <p>Criamos um dom\u00ednio chamado \"AlunosDomain\", um projeto pra cada aluno chamado \"KitJDaniel\" e \"KitJVitor\", e dois usu\u00e1rios, um chamado \"Daniel\" e outro \"Vitor\", adicionamos emails/senhas e atribu\u00edmos pap\u00e9is de \"admin\" para o dom\u00ednio e projeto.</p>"},{"location":"roteiro4/main/#aplicativo","title":"Aplicativo","text":"<p>Para criar o aplicativo, vamos utilizar o Terraform. Para cada Aluno, criamos uma pasta com o nome do aluno, e dentro dela criamos alguns arquivos para criar a infraestrutura desejada.</p> <p>Primeiro, vamos criar um arquivo chamado <code>provider.tf</code>, que \u00e9 o arquivo que cont\u00e9m as informa\u00e7\u00f5es de conex\u00e3o com o OpenStack. Esse arquivo deve conter as seguintes informa\u00e7\u00f5es:</p> <pre><code>terraform {\n  required_providers {\n    openstack = {\n      source  = \"terraform-provider-openstack/openstack\"\n      version = \"~&gt; 3.0.0\"\n    }\n  }\n}\n\nprovider \"openstack\" {\n  region              = \"RegionOne\"\n  user_name           = \"SEU_USUARIO\"\n}\n</code></pre> <p>Agora, vamos criar um arquivo chamado <code>instance1.tf</code>, que \u00e9 o arquivo que cont\u00e9m as informa\u00e7\u00f5es da inst\u00e2ncia que queremos criar. Esse arquivo deve conter as seguintes informa\u00e7\u00f5es:</p> <pre><code>resource \"openstack_compute_instance_v2\" \"basic\" {\n  name            = \"basic\"\n  image_id        = \"d9d79a26-88a1-4e63-8c07-4c733a0b94c2\"\n  flavor_id       = \"3bd591b8-7cde-458c-922a-52fec6bf6a36\"\n  key_pair        = \"user1\"\n  security_groups = [\"default\"]\n\n  network {\n    name = \"network_1\"\n  }\n}\n</code></pre> <p>OBS: Cada aluno criou um nome diferente para a rede, ent\u00e3o \u00e9 necess\u00e1rio alterar o nome da rede para o nome que voc\u00ea criou. Para isso, basta alterar a linha <code>name = \"network_1\"</code> para o nome da rede que voc\u00ea criou.</p> <p>Fizemos a mesma coisa para o arquivo <code>instance2.tf</code>, que \u00e9 o arquivo que cont\u00e9m as informa\u00e7\u00f5es da inst\u00e2ncia que queremos criar. Esse arquivo deve conter as seguintes informa\u00e7\u00f5es:</p> <p>Para o router, vamos criar um arquivo chamado <code>router.tf</code>, que \u00e9 o arquivo que cont\u00e9m as informa\u00e7\u00f5es do roteador que queremos criar. Esse arquivo deve conter as seguintes informa\u00e7\u00f5es:</p> <pre><code>  GNU nano 6.2                                                        router.tf                                                                 resource \"openstack_networking_router_v2\" \"router_2\" {\n  name                = \"user2_router\"\n  admin_state_up      = true\n  external_network_id = \"dc9a9a31-6c93-4eb5-9525-9db2cfe89b4d\"\n}\n\nresource \"openstack_networking_router_interface_v2\" \"int_2\" {\n  router_id = openstack_networking_router_v2.router_2.id\n  subnet_id = openstack_networking_subnet_v2.subnet_2.id\n}\n</code></pre> <p>Por fim, com o terraform instalado e os arquivos criados, vamos criar a infraestrutura. Para isso, basta executar os seguintes comandos:</p> <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre> <p>Ap\u00f3s executar o comando <code>terraform apply</code>, o Terraform ir\u00e1 criar a infraestrutura desejada. Para verificar se a infraestrutura foi criada corretamente, basta acessar o dashboard do OpenStack e verificar se as inst\u00e2ncias foram criadas corretamente.</p>"},{"location":"roteiro4/main/#exercicios-alunos-em-conjunto","title":"Exerc\u00edcios Alunos em Conjunto","text":"<p>Aba Identity projects no OpenStack</p> <p></p> <p>Aba Identity users no OpenStack</p>"},{"location":"roteiro4/main/#exercicios-aluno-1","title":"Exerc\u00edcios Aluno 1","text":"<p>Aba compute overview no OpenStack Dashboard</p> <p></p> <p>Aba compute instances no OpenStack Dashboard</p> <p></p> <p>Aba network topology no OpenStack Dashboard</p>"},{"location":"roteiro4/main/#exercicios-aluno-2","title":"Exerc\u00edcios Aluno 2","text":"<p>Aba compute overview no OpenStack Dashboard</p> <p></p> <p>Aba compute instances no OpenStack Dashboard</p> <p></p> <p>Aba network topology no OpenStack Dashboard</p>"},{"location":"roteiro4/main/#plano-de-disaster-recovery-e-sla-exercicio","title":"Plano de Disaster Recovery e SLA (Exerc\u00edcio)","text":"<p>Considerando um cen\u00e1rio em que somos o CTO (Chief Technology Officer) de uma grande empresa com sede em v\u00e1rias capitais no Brasil e precisamos implantar um sistema cr\u00edtico, de baixo custo e com dados sigilosos para a \u00e1rea operacional.</p> <ol> <li> <p>Public Cloud ou Private Cloud?</p> <ul> <li>A empresa deve optar por uma Private Cloud, pois os dados s\u00e3o sigilosos e a empresa n\u00e3o pode correr o risco de vazamento de dados. Al\u00e9m disso, a empresa possui um grande n\u00famero de servidores e pode manter uma infraestrutura pr\u00f3pria.</li> </ul> </li> <li> <p>Por que a empresa precisa de um time de DevOps?</p> <ul> <li>A empresa precisa de um time de DevOps para garantir que a infraestrutura esteja sempre dispon\u00edvel e funcionando corretamente. Al\u00e9m disso, o time de DevOps \u00e9 respons\u00e1vel por automatizar os processos de implanta\u00e7\u00e3o e gerenciamento da infraestrutura, garantindo agilidade e efici\u00eancia.</li> </ul> </li> <li> <p>Plano de DR (Disaster Recovery) e HA (Alta Disponibilidade)</p> <ul> <li>As principais amea\u00e7as s\u00e3o: </li> <li> <p>Queda de energia ou falha de hardware em datacenters regionais</p> </li> <li> <p>Ataques cibern\u00e9ticos como ransomware ou vazamento de dados operacionais</p> </li> <li> <p>Erros humanos em atualiza\u00e7\u00f5es manuais ou configura\u00e7\u00f5es incorretas</p> </li> <li> <p>Falhas de rede</p> </li> <li> <p>Indisponibilidade de recursos de cloud ou fornecedores externos</p> </li> </ul> </li> <li> <p>A\u00e7\u00f5es para a recupera\u00e7\u00e3o de seu ambiente em uma poss\u00edvel interrup\u00e7\u00e3o/desastre</p> <ul> <li> <p>Backup di\u00e1rio dos dados e configura\u00e7\u00e3o da infraestrutura</p> </li> <li> <p>Monitoramento constante da infraestrutura e dos servi\u00e7os</p> </li> <li> <p>Testes peri\u00f3dicos de recupera\u00e7\u00e3o de desastres</p> </li> <li> <p>Treinamento da equipe para lidar com situa\u00e7\u00f5es de emerg\u00eancia</p> </li> </ul> </li> <li> <p>Pol\u00edtica de backup</p> <ul> <li>A pol\u00edtica de backup deve ser definida de acordo com a criticidade dos dados e servi\u00e7os. Os backups devem ser realizados diariamente e armazenados em locais diferentes da infraestrutura principal. Al\u00e9m disso, os backups devem ser testados periodicamente para garantir que est\u00e3o funcionando corretamente.   </li> </ul> </li> <li> <p>Alta disponibilidade implementada na infraestrutura</p> <ul> <li>A alta disponibilidade deve ser implementada por meio de redund\u00e2ncia de hardware e software. Isso pode ser feito por meio de clusters de servidores, balanceadores de carga e replica\u00e7\u00e3o de dados. Al\u00e9m disso, a infraestrutura deve ser monitorada constantemente para garantir que os servi\u00e7os estejam sempre dispon\u00edveis.</li> </ul> </li> </ol>"}]}