{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-j","title":"KIT-J","text":"<p>Daniel Djanikian</p> <p>Vitor Padovani</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2 - Data 13/03/2025</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>O principal objetivo deste roteiro \u00e9 aprender a configurar um ambiente de cloud bare-metal. Durante o processo, ser\u00e3o abordados os seguintes pontos:</p> <p>Configura\u00e7\u00e3o de Infraestrutura: Montar a subrede para comunica\u00e7\u00e3o entre os servidores, configurando o ambiente com Ubuntu e utilizando o MaaS para gerenciar o hardware e a rede.</p> <p>Implanta\u00e7\u00e3o e Integra\u00e7\u00e3o de Servi\u00e7os: Realizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o de servi\u00e7os essenciais, como o banco de dados PostgreSQL e a aplica\u00e7\u00e3o Django, incluindo a implementa\u00e7\u00e3o de deploy manual e automatizado com Ansible.</p> <p>Implementa\u00e7\u00e3o de Conectividade e Balanceamento de Carga: Configurar roteadores, DHCP e proxy reverso com NGINX para assegurar a conectividade interna/externa e distribuir a carga entre os servidores.</p>"},{"location":"roteiro1/main/#material-utilizado","title":"Material Utilizado","text":"<p>1 NUC (main) com 10Gb e 1 SSD (120Gb)</p> <p>1 NUC (server1) com 12Gb e 1 SSD (120Gb)</p> <p>1 NUC (server2) com 16Gb e 2 SSD (120Gb+120Gb)</p> <p>3 NUCs (server3, server4 e server5) com 32Gb e 2 SSD (120Gb+120Gb)</p> <p>1 Switch DLink DSG-1210-28 de 28 portas</p> <p>1 Roteador TP-Link TL-R470T+</p>"},{"location":"roteiro1/main/#criando-e-usando-a-infraestrutura","title":"Criando e Usando a Infraestrutura","text":"<p>O objetivo desta etapa \u00e9 preparar a rede f\u00edsica e l\u00f3gica, garantindo que todos os NUCs estejam na mesma subrede e tenham acesso \u00e0 rede externa via roteador. Uma m\u00e1quina principal, chamada de main, ser\u00e1 configurada com o MaaS para gerenciar todas as demais m\u00e1quinas. A seguir, os passos realizados:</p>"},{"location":"roteiro1/main/#1-conexao-dos-dispositivos-e-instalacao-do-ubuntu-server","title":"1. Conex\u00e3o dos Dispositivos e Instala\u00e7\u00e3o do Ubuntu Server","text":"<ul> <li> <p>Conex\u00e3o F\u00edsica:   Todos os NUCs e o roteador s\u00e3o conectados ao switch, formando a base da rede local.</p> </li> <li> <p>Instala\u00e7\u00e3o do Sistema Operacional:   No NUC principal (main), instalamos o Ubuntu Server 22.04.5 LTS para garantir estabilidade e compatibilidade.  </p> </li> <li> <p>Configuramos um IP est\u00e1tico para assegurar o acesso remoto cont\u00ednuo e evitar mudan\u00e7as din\u00e2micas que poderiam comprometer a comunica\u00e7\u00e3o.</p> </li> <li> <p>Instala\u00e7\u00e3o do MaaS:   Utilizamos o MaaS (vers\u00e3o 3.5) para orquestrar e gerenciar o hardware do NUC main.</p> </li> </ul> sudo snap install maas --channel=3.5/Stable <ul> <li>Configura\u00e7\u00e3o Inicial do MaaS: </li> <li>Inicializamos o MaaS com a URL e o banco de dados de teste.  </li> </ul> sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:/// <ul> <li>Criamos o administrador utilizando o login cloud e a senha padr\u00e3o da disciplina.  </li> <li>Habilitamos o acesso remoto via SSH para facilitar a administra\u00e7\u00e3o (veja a se\u00e7\u00e3o de SSH abaixo).  </li> <li>Acessamos o Dashboard do MaaS pelo endere\u00e7o <code>http://172.16.0.3:5240/MAAS</code> e importamos as imagens do Ubuntu (22.04 LTS e 20.04 LTS), para poderem ser utilizadas na instala\u00e7\u00e3o das outras NUCs.  </li> <li>Configuramos o DNS Forwarder para utilizar o DNS externo do Insper.  </li> <li> <p>Em Settings | General, ajustamos os par\u00e2metros do kernel, definindo <code>net.ifnames=0</code>.</p> </li> <li> <p>Configura\u00e7\u00e3o do DHCP no MaaS:</p> </li> <li>Como o switch n\u00e3o tem o servi\u00e7o DHCP, ou seja, ele n\u00e3o consegue entregar IPs aos dispositivos na rede, vamos utilizar o roteador para isso.</li> <li>Habilitamos o DHCP na subrede configurada pelo MaaS Controller e ajustamos o Reserved Range para iniciar em 172.16.11.1 e terminar em 172.16.14.255, dentro da sub-rede definida pela m\u00e1scara 255.255.240.0. Essa configura\u00e7\u00e3o permite que os endere\u00e7os atribu\u00eddos pelo DHCP fiquem restritos a uma parte do espa\u00e7o de endere\u00e7amento dispon\u00edvel, ajudando a controlar e organizar os acessos dentro da rede.  </li> <li> <p>Desabilitamos o DHCP no roteador, para que o MaaS seja o respons\u00e1vel pela distribui\u00e7\u00e3o dos IPs.</p> </li> <li> <p>Verifica\u00e7\u00f5es de Conectividade:   Validamos a configura\u00e7\u00e3o de rede realizando pings para <code>8.8.8.8</code> e <code>www.google.com</code>, assegurando que o roteamento dos pacotes e a resolu\u00e7\u00e3o de URLs estejam funcionando corretamente.</p> </li> </ul>"},{"location":"roteiro1/main/#o-que-e-o-ssh-e-como-ele-foi-configurado","title":"O que \u00e9 o SSH e como ele foi Configurado","text":"<p>SSH (Secure Shell) \u00e9 um protocolo que permite o acesso remoto seguro a sistemas, criando um canal criptografado entre o cliente e o servidor. Isso garante que as informa\u00e7\u00f5es transmitidas, como comandos e credenciais, estejam protegidas contra intercepta\u00e7\u00f5es.</p> <ol> <li> <p>Gera\u00e7\u00e3o do Par de Chaves:    Utilizamos <code>ssh-keygen -t -rsa</code> para criar um par de chaves (p\u00fablica e privada), permitindo a autentica\u00e7\u00e3o sem a necessidade de enviar senhas em texto claro.</p> </li> <li> <p>Distribui\u00e7\u00e3o da Chave P\u00fablica:    A chave p\u00fablica gerada foi copiada para o servidor (NUC main), possibilitando que o servidor autentique o cliente que possui a chave privada correspondente. Vale ressaltar que esse servi\u00e7o do ssh trabalha na porta 22.</p> </li> </ol>"},{"location":"roteiro1/main/#3-comissionamento-dos-servidores-e-criacao-de-ovs-bridge","title":"3. Comissionamento dos Servidores e Cria\u00e7\u00e3o de OVS Bridge","text":"<ul> <li>Comissionamento dos Servidores:   No Dashboard do MaaS, cadastramos os hosts (server1 at\u00e9 server5) e configuramos a op\u00e7\u00e3o de Power Type para Intel AMT. Foram inseridos os seguintes detalhes:  </li> <li>MAC Address (anotado previamente).  </li> <li>Senha padr\u00e3o: <code>CloudComp6s!</code>.  </li> <li>IP do AMT, no formato <code>172.16.15.X</code> (onde X corresponde ao id do servidor, por exemplo, server1 = 172.16.15.1).   Ap\u00f3s o boot via PXE, os servidores devem aparecer com o status \"Ready\", indicando que os par\u00e2metros de hardware (CPU, mem\u00f3ria, SSD e rede) foram detectados corretamente.   Tamb\u00e9m adicionamos o roteador como device no Dashboard do MaaS.</li> </ul> <p>Obs: O server 1 da nossa cloud estava enfrentando problemas para ser encontrado. Como esse roteiro n\u00e3o necessitava a utiliza\u00e7\u00e3o de todos os servers, seguimos utilizando os demais.</p> <ul> <li>Cria\u00e7\u00e3o de OVS Bridge:   Para reduzir a necessidade de duas interfaces de rede f\u00edsicas, configuramos uma Open vSwitch (OVS) bridge.  </li> <li>A ponte \u00e9 criada a partir da interface padr\u00e3o <code>enp1s0</code> e nomeada br-ex.  </li> <li>Essa configura\u00e7\u00e3o \u00e9 aplicada em todos os cinco n\u00f3s, garantindo flexibilidade e suporte ao OVN Chassis.</li> </ul>"},{"location":"roteiro1/main/#4-configuracao-de-nat-e-acesso-remoto","title":"4. Configura\u00e7\u00e3o de NAT e Acesso Remoto","text":""},{"location":"roteiro1/main/#o-que-e-nat","title":"O que \u00e9 NAT?","text":"<p>O NAT (Network Address Translation) \u00e9 uma t\u00e9cnica que permite que dispositivos de uma rede privada compartilhem um \u00fanico endere\u00e7o IP p\u00fablico para acessar a internet. Ele realiza a tradu\u00e7\u00e3o dos endere\u00e7os IP privados para um endere\u00e7o p\u00fablico e vice-versa, garantindo que os pacotes de dados sejam direcionados corretamente entre a rede interna e a externa.</p>"},{"location":"roteiro1/main/#como-o-nat-funciona-na-nossa-configuracao","title":"Como o NAT Funciona na Nossa Configura\u00e7\u00e3o","text":"<ul> <li> <p>Tradu\u00e7\u00e3o de Endere\u00e7os:   Cada dispositivo na rede local possui um endere\u00e7o IP privado (por exemplo, na faixa 172.16.0.0/20). Quando um dispositivo envia dados para a internet, o roteador que realiza o NAT substitui o endere\u00e7o IP privado pelo endere\u00e7o IP p\u00fablico configurado. Dessa forma, o tr\u00e1fego de sa\u00edda parece originar de um \u00fanico IP p\u00fablico.</p> </li> <li> <p>Port Forwarding para Acesso Remoto:   Para permitir a conex\u00e3o remota ao servidor principal (main) pela porta 22, configuramos um redirecionamento de porta (port forwarding). Isso faz com que qualquer conex\u00e3o que chegue \u00e0 porta 22 do endere\u00e7o IP p\u00fablico seja encaminhada para o IP fixo do servidor main (172.16.0.3).</p> </li> </ul>"},{"location":"roteiro1/main/#configuracao-do-nat-e-das-regras-de-gerenciamento-no-roteador","title":"Configura\u00e7\u00e3o do NAT e das Regras de Gerenciamento no Roteador","text":"<ul> <li> <p>Acesso \u00e0 Interface do Roteador:   A configura\u00e7\u00e3o \u00e9 realizada atrav\u00e9s da interface administrativa do roteador, acess\u00edvel via navegador atrav\u00e9s do IP do dispositivo.</p> </li> <li> <p>Defini\u00e7\u00e3o das Regras de NAT:   Na interface do roteador, configuramos as seguintes regras:</p> <ol> <li>Regra de NAT: Define que todos os dispositivos da subrede (172.16.0.0/20) usem o endere\u00e7o IP p\u00fablico do roteador para acessar a internet.</li> <li> <p>Port Forwarding: Cria uma regra para redirecionar conex\u00f5es que chegam na porta 22 para o IP 172.16.0.3, permitindo o acesso remoto seguro ao servidor main.</p> </li> <li> <p>Regras de Gerenciamento Remoto: Adicionalmente, foi criada uma regra que permite o acesso remoto ao pr\u00f3prio roteador (regra de gest\u00e3o), configurada para aceitar conex\u00f5es de qualquer endere\u00e7o (0.0.0.0/0). Essa regra possibilita a administra\u00e7\u00e3o do roteador remotamente.</p> </li> </ol> </li> </ul> <p>Essa configura\u00e7\u00e3o garante que a rede local tenha acesso \u00e0 internet por meio de um \u00fanico endere\u00e7o IP p\u00fablico, ao mesmo tempo que permite o gerenciamento remoto seguro tanto do servidor main quanto do roteador.</p>"},{"location":"roteiro1/main/#bare-metal-aplicacao","title":"Bare Metal - Aplica\u00e7\u00e3o","text":"<p>Nesta etapa, realizamos o deploy manual de uma aplica\u00e7\u00e3o simples em Django utilizando a infraestrutura configurada anteriormente na nuvem MaaS. \u00c9 importante notar que, devido a problemas t\u00e9cnicos, o server originalmente designado como \"server1\" n\u00e3o funcionou corretamente. Dessa forma, o servidor que inicialmente seria o server1 passou a ser considerado como server2, o que acarretou um reajuste na numera\u00e7\u00e3o dos demais servidores (server2 passou a ser server3, e assim por diante).</p> <p>Al\u00e9m disso, durante a instala\u00e7\u00e3o das imagens nos servidores, encontramos um problema inesperado: a Canonical atualizou a imagem do Ubuntu, alterando algum componente essencial e rompendo a compatibilidade. Para resolver esse problema, foi necess\u00e1rio atualizar o firmware em todas as m\u00e1quinas para que o Ubuntu 22.04 funcionasse corretamente.</p>"},{"location":"roteiro1/main/#1-ajuste-no-dns-do-servidor","title":"1. Ajuste no DNS do Servidor","text":"<p>Antes de iniciar o deploy, foi preciso ajustar a configura\u00e7\u00e3o do DNS:</p> <ul> <li>Acesse a aba Subnets no MaaS e edite a subnet <code>172.16.0.0/20</code>, alterando o campo Subnet summary para usar o DNS do Insper (<code>172.20.129.131</code>).</li> </ul>"},{"location":"roteiro1/main/#2-primeira-parte-configuracao-do-banco-de-dados","title":"2. Primeira Parte: Configura\u00e7\u00e3o do Banco de Dados","text":"<p>Utilizamos o PostgreSQL, um servidor de banco de dados robusto e amplamente utilizado em projetos open source, conforme os passos abaixo:</p> <ul> <li>Deploy do Ubuntu:   No MaaS, foi realizado o deploy do Ubuntu 22.04 no servidor designado (que originalmente seria o server1, mas considerando o ajuste, este \u00e9 agora o server2).</li> <li>Instala\u00e7\u00e3o e Configura\u00e7\u00e3o do PostgreSQL:   No terminal do servidor (acessado via SSH), executamos os seguintes procedimentos:</li> <li>Atualiza\u00e7\u00e3o do sistema.</li> </ul> sudo apt update <ul> <li>Instala\u00e7\u00e3o dos pacotes do PostgreSQL e suas contribui\u00e7\u00f5es.</li> </ul> sudo apt install postgresql postgresql-contrib -y <ul> <li>Cria\u00e7\u00e3o de um usu\u00e1rio para a aplica\u00e7\u00e3o (usu\u00e1rio <code>cloud</code> com senha <code>cloud</code>).</li> </ul> sudo su - postgrescreateuser -s cloud -W <ul> <li>Cria\u00e7\u00e3o de uma base de dados (por exemplo, <code>tasks</code>).</li> </ul> createdb -O cloud tasks <ul> <li>Edi\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o do PostgreSQL para que o servi\u00e7o aceite conex\u00f5es remotas (definindo <code>listen_addresses = '*'</code>).</li> </ul> nano /etc/postgresql/14/main/postgresql.conf <ul> <li>Ajuste no arquivo de controle de acesso (<code>pg_hba.conf</code>) para liberar conex\u00f5es vindas da subnet <code>172.16.0.0/20</code>.</li> <li>Libera\u00e7\u00e3o da porta (5432) no firewall e reinicializa\u00e7\u00e3o do servi\u00e7o.</li> </ul> sudo ufw allow 5432/tcpsudo systemctl restart postgresql"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Verificando se o banco est\u00e1 funcionando</p> <p></p> <p>Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional</p> <p></p> <p>Verifica se iniciou sem erro </p> <p></p> <p>Acessivel na pr\u00f3pria maquina na qual ele foi implantado</p> <p></p> <p>Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN na porta 5432.</p>"},{"location":"roteiro1/main/#3-parte-ii-deploy-da-aplicacao-django","title":"3. Parte II: Deploy da Aplica\u00e7\u00e3o Django","text":"<ul> <li>Reserva e Deploy da M\u00e1quina:   Utilizando o MaaS CLI, reservamos uma m\u00e1quina (originalmente designada como server2, mas, com o ajuste, ela se torna o novo server3 conforme a sequ\u00eancia), e realizamos o deploy atrav\u00e9s do CLI do MaaS. Vale ressaltar que, por conta do problema com a imagem, tivemos que refazer todos os deploys manualmente pelo MaaS.</li> <li>Clone e Instala\u00e7\u00e3o da Aplica\u00e7\u00e3o:   Acessamos o servidor via SSH, clonamos o reposit\u00f3rio da aplica\u00e7\u00e3o Django e executamos o script de instala\u00e7\u00e3o (<code>install.sh</code>).</li> </ul> maas [login] machine deploy [system_id]git clone https://github.com/raulikeda/tasks.git./install.shsudo reboot <ul> <li> <p>Instalando e configurando o Django (Parte da Tarefa 3):</p> <p>Ap\u00f3s o deploy manual da nossa m\u00e1quina, por conta do erro, para baixar a aplica\u00e7\u00e3o do Django, entramos no server3 e rodamos os seguintes comandos para instalar o Django:</p> <p>sudo apt install python3-psycopg2pip install django --break-system-packages </p> <p>Al\u00e9m disso, para a aplica\u00e7\u00e3o funcionar, modificamos o arquivo <code>settings.py</code> do Django para indicar que o banco de dados que iremos utilizar estar\u00e1 no server2 e ajustamos os <code>ALLOWED_HOSTS</code> para permitir o acesso sem problemas a partir do main.</p> <p>nano portfolio/settings.py </p> </li> <li> <p>Fazendo com que o server3 Enxergue o server2</p> <p>Como apenas o servidor main consegue visualizar todas as m\u00e1quinas (devido \u00e0 sua fun\u00e7\u00e3o de controlador do MaaS), precisamos informar o server3 sobre a localiza\u00e7\u00e3o do server2, que hospeda o PostgreSQL para a aplica\u00e7\u00e3o Django. Para isso, editamos o arquivo <code>/etc/hosts</code> no server3 e adicionamos uma entrada que associa o nome (\"server2\") ao respectivo endere\u00e7o IP.</p> <p>Essa a\u00e7\u00e3o garante que, mesmo sem acesso direto \u00e0 tabela de DNS da rede, o server3 consiga resolver o nome do server2 e estabelecer comunica\u00e7\u00e3o com ele. Al\u00e9m disso, desabilitamos a configura\u00e7\u00e3o para que essa altera\u00e7\u00e3o n\u00e3o seja perdida ap\u00f3s uma reinicializa\u00e7\u00e3o.</p> <p>sudo nano /etc/hosts </p> </li> </ul> <p>Teste da Aplica\u00e7\u00e3o:   Ap\u00f3s reiniciar a m\u00e1quina, testamos o acesso \u00e0 aplica\u00e7\u00e3o via terminal do MaaS utilizando um comando para verificar a porta 8080.</p> wget http://172.16.0.15:8080/admin/ <p>Para acesso no browser, criamos um t\u00fanel SSH redirecionando a porta 8080 do servidor para a porta 8001 local.</p> ssh cloud@10.103.1.19 -L 8001:172.16.0.15:8080 <p>Esse t\u00fanel faz com que utilizemos a porta 8001 do computador local para acessar o que est\u00e1 na porta 8080 do server3. Isso significa que qualquer solicita\u00e7\u00e3o feita para http://localhost:8001/admin/ ser\u00e1 redirecionada para http://172.16.0.15:8080/admin/. Dessa forma, podemos acessar a interface de administra\u00e7\u00e3o do django do server3 localmente atrav\u00e9s do navegador web.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<p>Dashboard do MAAS com Server 2 e 3 deployados</p> <p></p> <p>Comprova\u00e7\u00e3o das imagens do Ubuntu sincronizadas</p> <p> </p> <p>Todas as m\u00e1quinas passaram nos testes de hardware e commissioning com status \"OK\"</p>"},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<p>Dashboard do MAAS com os IPS das m\u00e1quinas</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server3</p> <p></p> <p>Comprovando que estamos rodando no server3</p>"},{"location":"roteiro1/main/#4-deploy-automatizado-com-ansible","title":"4. Deploy Automatizado com Ansible","text":"<p>O Ansible \u00e9 uma ferramenta de automa\u00e7\u00e3o que permite gerenciar configura\u00e7\u00f5es e implanta\u00e7\u00f5es de forma padronizada e repet\u00edvel. Diferente do processo manual, ele garante que os passos de instala\u00e7\u00e3o sejam executados da mesma forma em todas as m\u00e1quinas, sem interferir em estados intermedi\u00e1rios \u2013 um conceito conhecido como idempot\u00eancia.</p>"},{"location":"roteiro1/main/#o-que-acontece-durante-o-deploy-automatizado","title":"O Que Acontece Durante o Deploy Automatizado?","text":"<ol> <li> <p>Provisionamento da M\u00e1quina de Destino:    Ap\u00f3s solicitar o deploy via MaaS, uma nova m\u00e1quina (agora designada como server4) \u00e9 alocada para a aplica\u00e7\u00e3o.</p> </li> <li> <p>Instala\u00e7\u00e3o do Ansible no Servidor Principal:    Instalamos o Ansible no servidor principal (main). Isso transforma o main em um controlador central que se conecta \u00e0s m\u00e1quinas remotas para executar os comandos necess\u00e1rios.</p> </li> </ol> sudo apt install ansible <ol> <li>Baixar o Playbook:    Um playbook \u00e9 um arquivo YAML que cont\u00e9m uma s\u00e9rie de instru\u00e7\u00f5es que definem como a aplica\u00e7\u00e3o Django deve ser instalada e configurada. Ao baixar esse arquivo, garantimos que todos os passos necess\u00e1rios ser\u00e3o executados de forma padronizada.</li> </ol> wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml <ol> <li>Execu\u00e7\u00e3o do Playbook:    Com o playbook em m\u00e3os, utilizamos o comando do Ansible para execut\u00e1-lo, passando o IP do server4 como vari\u00e1vel extra. Esse processo automatiza a instala\u00e7\u00e3o da aplica\u00e7\u00e3o Django, realizando as mesmas a\u00e7\u00f5es que seriam feitas manualmente (como instala\u00e7\u00e3o de pacotes, configura\u00e7\u00e3o de servi\u00e7os e deploy da aplica\u00e7\u00e3o), mas sem a necessidade de interven\u00e7\u00e3o manual em cada servidor.</li> </ol> ansible-playbook tasks-install-playbook.yaml --extra-vars server=172.16.0.17 <ol> <li> <p>Fazendo com que o server4 enxergue o server2</p> <p>Seguindo o mesmo processo feito para o server3, devemos fazer para o server4 enxergar o server2. Para isso entramos nas configura\u00e7\u00d5es do <code>/etc/hosts</code> no server3 e adicionamos uma entrada que associa o nome (\"server2\") ao respectivo endere\u00e7o IP.</p> </li> </ol>"},{"location":"roteiro1/main/#por-que-usar-o-ansible","title":"Por Que Usar o Ansible?","text":"<ul> <li> <p>Idempot\u00eancia:   O Ansible garante que, mesmo se executarmos o playbook m\u00faltiplas vezes, o sistema sempre alcan\u00e7ar\u00e1 o estado desejado, sem efeitos colaterais indesejados.</p> </li> <li> <p>Gerenciamento Simult\u00e2neo de V\u00e1rias M\u00e1quinas:   Permite que um \u00fanico playbook seja aplicado a m\u00faltiplos servidores de uma vez, garantindo uniformidade na instala\u00e7\u00e3o e configura\u00e7\u00e3o da aplica\u00e7\u00e3o.</p> </li> <li> <p>Facilidade de Automa\u00e7\u00e3o:   Com um arquivo de configura\u00e7\u00e3o centralizado, qualquer altera\u00e7\u00e3o ou corre\u00e7\u00e3o pode ser aplicada de maneira r\u00e1pida e consistente em todos os n\u00f3s.</p> </li> <li> <p>Redu\u00e7\u00e3o de Erros Humanos:   Automatizando o processo, minimizamos a chance de erros que podem ocorrer durante a configura\u00e7\u00e3o manual de cada servidor.</p> </li> </ul>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<p>Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server3</p> <p></p> <p>Comprovando que estamos rodando no server3</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server4</p> <p></p> <p>Comprovando que estamos rodando no server4</p> <p>A diferen\u00e7a entre instalar manualmente a aplica\u00e7\u00e3o Django e utilizar o Ansible \u00e9 que, na instala\u00e7\u00e3o manual, cada comando deve ser executado individualmente em cada servidor, o que aumenta o risco de erros e pode resultar em configura\u00e7\u00f5es inconsistentes. J\u00e1 com o Ansible, um playbook automatiza todo o processo, garantindo que os mesmos passos sejam aplicados de forma id\u00eantica em todos os n\u00f3s, tornando o deploy mais r\u00e1pido, confi\u00e1vel e escal\u00e1vel, al\u00e9m de reduzir significativamente a possibilidade de erros humanos.</p>"},{"location":"roteiro1/main/#5-balancamento-de-carga-com-proxy-reverso","title":"5. Balancamento de Carga com Proxy Reverso","text":"<p>O balanceamento de carga com proxy reverso tem como objetivo centralizar o acesso \u00e0 aplica\u00e7\u00e3o, de forma que uma \u00fanica entrada redirecione as requisi\u00e7\u00f5es para v\u00e1rios servidores que hospedam a aplica\u00e7\u00e3o Django. Essa abordagem \u00e9 essencial para garantir alta disponibilidade e redund\u00e2ncia: se um dos servidores falhar, os outros continuam atendendo as requisi\u00e7\u00f5es, mantendo a estabilidade do servi\u00e7o.</p> <p>Nesse cen\u00e1rio, instalamos o NGINX no server5 para atuar como proxy reverso. O primeiro passo \u00e9 instalar o NGINX, utilizando o gerenciador de pacotes do sistema:</p> sudo apt-get install nginx <p>Ap\u00f3s a instala\u00e7\u00e3o, editamos o arquivo de configura\u00e7\u00e3o do NGINX, localizado em <code>/etc/nginx/sites-available/default</code>. Nele, definimos o bloco upstream que aponta para os servidores backend onde a aplica\u00e7\u00e3o Django est\u00e1 rodando:</p> <pre><code>upstream backend { server 172.16.0.15:8080; server 172.16.0.17:8080; }\n</code></pre> <p>Em seguida, configuramos o servidor virtual para encaminhar todas as requisi\u00e7\u00f5es que chegarem na porta 80 para o grupo de servidores definido acima, utilizando a diretiva <code>proxy_pass</code>:</p> <pre><code>server { location / { proxy_pass http://backend; } }\n</code></pre> <p>Al\u00e9m disso comentamos algumas linhas para que n\u00e3o houvesse conflitos das informa\u00e7\u00d5es.</p> <p>Ap\u00f3s todas as configura\u00e7\u00f5es realizadas no nginx, reiniciamos o servi\u00e7o</p> sudo service nginx restart <p>A partir deste momento, o NGINX distribuir\u00e1 as requisi\u00e7\u00f5es recebidas entre os servidores definidos no bloco upstream, utilizando o algoritmo Round Robin por padr\u00e3o. Isso melhora a escalabilidade e a toler\u00e2ncia a falhas, pois se um dos servidores ficar indispon\u00edvel, o outro continuar\u00e1 atendendo \u00e0s requisi\u00e7\u00f5es.</p> <p>Para verificar o funcionamento, modificamos a fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> em cada inst\u00e2ncia da aplica\u00e7\u00e3o Django, atribuindo mensagens distintas. Assim, ao acessar a aplica\u00e7\u00e3o por meio do server5, podemos observar que as respostas alternam entre as mensagens, confirmando que o balanceamento de carga est\u00e1 ativo e direcionando as requisi\u00e7\u00f5es para cada servidor de forma rotativa.</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<p>Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs</p> <p></p> <p>Mudan\u00e7a da mensagem no views.py server3</p> <p></p> <p>Mudan\u00e7a da mensagem no views.py server4</p> <p></p> <p>Requisi\u00e7\u00e3o com print da mensagem do server3</p> <p></p> <p>Requisi\u00e7\u00e3o com print da mensagem do server4</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Durante a configura\u00e7\u00e3o e implementa\u00e7\u00e3o da nuvem bare-metal, surgiram diversos pontos de discuss\u00e3o e aprendizado:</p> <ul> <li> <p>Configura\u00e7\u00e3o de Rede e MaaS: Embora o MaaS simplifique a orquestra\u00e7\u00e3o de hardware, entender sua l\u00f3gica de DHCP e a intera\u00e7\u00e3o com o roteador demandou um tempo consider\u00e1vel. O fato de o switch n\u00e3o possuir DHCP embutido exigiu aten\u00e7\u00e3o redobrada na defini\u00e7\u00e3o dos endere\u00e7os IP e no desvio de fun\u00e7\u00f5es do roteador para o MaaS.  </p> </li> <li> <p>Problemas T\u00e9cnicos e Solu\u00e7\u00f5es: A indisponibilidade do servidor que originalmente seria o server1 mostrou a import\u00e2ncia de ter planos de conting\u00eancia. O ajuste na numera\u00e7\u00e3o dos servidores e a atualiza\u00e7\u00e3o de firmware para corrigir a compatibilidade com o Ubuntu 22.04 demonstram como ambientes de produ\u00e7\u00e3o podem exigir adapta\u00e7\u00f5es e reconfigura\u00e7\u00f5es inesperadas.</p> </li> <li> <p>Instala\u00e7\u00e3o Manual vs. Ansible: A instala\u00e7\u00e3o manual do Django e do banco de dados PostgreSQL, embora did\u00e1tica para fins de aprendizado, revelou-se propensa a erros e repeti\u00e7\u00f5es. Em contrapartida, o uso do Ansible permitiu automatizar a configura\u00e7\u00e3o de m\u00faltiplos servidores de forma consistente, evidenciando a escalabilidade e a confiabilidade proporcionadas pelas ferramentas de automa\u00e7\u00e3o.</p> </li> <li> <p>Balanceamento de Carga: Configurar o NGINX como proxy reverso enfatizou a import\u00e2ncia de uma \u00fanica entrada de acesso para distribuir requisi\u00e7\u00f5es entre v\u00e1rios servidores. Esse passo \u00e9 fundamental para garantir alta disponibilidade, especialmente em cen\u00e1rios de produ\u00e7\u00e3o que exigem redund\u00e2ncia e escalabilidade.</p> </li> <li> <p>Dificuldades e Facilidade: A maior dificuldade relatada foi lidar com ajustes de firmware e vers\u00f5es de imagem que mudaram subitamente. J\u00e1 a maior facilidade percebida foi a automa\u00e7\u00e3o do deploy com Ansible, que simplificou a replica\u00e7\u00e3o das configura\u00e7\u00f5es e reduziu erros humanos.</p> </li> </ul>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro proporcionou uma vis\u00e3o completa do processo de cria\u00e7\u00e3o e gest\u00e3o de uma nuvem bare-metal. Iniciando pela configura\u00e7\u00e3o da infraestrutura de rede e passando pela instala\u00e7\u00e3o manual de servi\u00e7os cr\u00edticos como o PostgreSQL e o Django, o aprendizado foi consolidado ao automatizar tarefas com Ansible e ao implementar um balanceador de carga via NGINX.</p> <p>No final, foi poss\u00edvel concluir que:</p> <ol> <li> <p>Gerenciamento Centralizado \u00e9 Essencial: O MaaS demonstrou ser uma ferramenta valiosa para orquestrar o provisionamento e o controle de m\u00faltiplos servidores, otimizando a aloca\u00e7\u00e3o de recursos e o gerenciamento de IPs.</p> </li> <li> <p>Automa\u00e7\u00e3o Reduz Erros e Garante Consist\u00eancia: A diferen\u00e7a entre instalar servi\u00e7os manualmente e utilizar o Ansible evidenciou o quanto a automa\u00e7\u00e3o traz confiabilidade, escalabilidade e facilidade de manuten\u00e7\u00e3o.</p> </li> <li> <p>Alta Disponibilidade e Redund\u00e2ncia: A configura\u00e7\u00e3o de um proxy reverso (NGINX) para balanceamento de carga exemplificou a import\u00e2ncia de garantir que a aplica\u00e7\u00e3o se mantenha dispon\u00edvel mesmo diante de falhas pontuais em um ou mais servidores.</p> </li> </ol> <p>Em suma, este roteiro possibilitou a experi\u00eancia pr\u00e1tica de construir uma infraestrutura de cloud bare-metal, destacando desde os fundamentos de rede at\u00e9 a entrega de aplica\u00e7\u00f5es de forma robusta e escal\u00e1vel, aproximando o aprendizado dos desafios reais encontrados em data centers e ambientes de produ\u00e7\u00e3o.</p>"}]}