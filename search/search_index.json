{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-j","title":"KIT-J","text":"<p>Daniel Djanikian</p> <p>Vitor Padovani</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2 - Data 13/03/2025</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>O principal objetivo deste roteiro \u00e9 aprender a configurar um ambiente de cloud bare-metal. Durante o processo, ser\u00e3o abordados os seguintes pontos:</p> <p>Configura\u00e7\u00e3o de Infraestrutura: Montar a subrede para comunica\u00e7\u00e3o entre os servidores, configurando o ambiente com Ubuntu e utilizando o MaaS para gerenciar o hardware e a rede.</p> <p>Implanta\u00e7\u00e3o e Integra\u00e7\u00e3o de Servi\u00e7os: Realizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o de servi\u00e7os essenciais, como o banco de dados PostgreSQL e a aplica\u00e7\u00e3o Django, incluindo a implementa\u00e7\u00e3o de deploy manual e automatizado com Ansible.</p> <p>Implementa\u00e7\u00e3o de Conectividade e Balanceamento de Carga: Configurar roteadores, DHCP e proxy reverso com NGINX para assegurar a conectividade interna/externa e distribuir a carga entre os servidores.</p>"},{"location":"roteiro1/main/#material-utilizado","title":"Material Utilizado","text":"<p>1 NUC (main) com 10Gb e 1 SSD (120Gb)</p> <p>1 NUC (server1) com 12Gb e 1 SSD (120Gb)</p> <p>1 NUC (server2) com 16Gb e 2 SSD (120Gb+120Gb)</p> <p>3 NUCs (server3, server4 e server5) com 32Gb e 2 SSD (120Gb+120Gb)</p> <p>1 Switch DLink DSG-1210-28 de 28 portas</p> <p>1 Roteador TP-Link TL-R470T+</p>"},{"location":"roteiro1/main/#criando-e-usando-a-infraestrutura","title":"Criando e Usando a Infraestrutura","text":"<p>O objetivo desta etapa \u00e9 preparar a rede f\u00edsica e l\u00f3gica, garantindo que todos os NUCs estejam na mesma subrede e tenham acesso \u00e0 rede externa via roteador. Uma m\u00e1quina principal, chamada de main, ser\u00e1 configurada com o MaaS para gerenciar todas as demais m\u00e1quinas. A seguir, os passos realizados:</p>"},{"location":"roteiro1/main/#1-conexao-dos-dispositivos-e-instalacao-do-ubuntu-server","title":"1. Conex\u00e3o dos Dispositivos e Instala\u00e7\u00e3o do Ubuntu Server","text":"<ul> <li> <p>Conex\u00e3o F\u00edsica:   Todos os NUCs e o roteador s\u00e3o conectados ao switch, formando a base da rede local.</p> </li> <li> <p>Instala\u00e7\u00e3o do Sistema Operacional:   No NUC principal (main), instalamos o Ubuntu Server 22.04.5 LTS para garantir estabilidade e compatibilidade.  </p> </li> <li> <p>Configuramos um IP est\u00e1tico para assegurar o acesso remoto cont\u00ednuo e evitar mudan\u00e7as din\u00e2micas que poderiam comprometer a comunica\u00e7\u00e3o.</p> </li> <li> <p>Instala\u00e7\u00e3o do MaaS:   Utilizamos o MaaS (vers\u00e3o 3.5) para orquestrar e gerenciar o hardware do NUC main.</p> </li> </ul> sudo snap install maas --channel=3.5/Stable <ul> <li>Configura\u00e7\u00e3o Inicial do MaaS: </li> <li>Inicializamos o MaaS com a URL e o banco de dados de teste.  </li> </ul> sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:/// <ul> <li>Criamos o administrador utilizando o login cloud e a senha padr\u00e3o da disciplina.  </li> <li>Habilitamos o acesso remoto via SSH para facilitar a administra\u00e7\u00e3o (veja a se\u00e7\u00e3o de SSH abaixo).  </li> <li>Acessamos o Dashboard do MaaS pelo endere\u00e7o <code>http://172.16.0.3:5240/MAAS</code> e importamos as imagens do Ubuntu (22.04 LTS e 20.04 LTS), para poderem ser utilizadas na instala\u00e7\u00e3o das outras NUCs.  </li> <li>Configuramos o DNS Forwarder para utilizar o DNS externo do Insper.  </li> <li> <p>Em Settings | General, ajustamos os par\u00e2metros do kernel, definindo <code>net.ifnames=0</code>.</p> </li> <li> <p>Configura\u00e7\u00e3o do DHCP no MaaS:</p> </li> <li>Como o switch n\u00e3o tem o servi\u00e7o DHCP, ou seja, ele n\u00e3o consegue entregar IPs aos dispositivos na rede, vamos utilizar o roteador para isso.</li> <li>Habilitamos o DHCP na subrede configurada pelo MaaS Controller e ajustamos o Reserved Range para iniciar em 172.16.11.1 e terminar em 172.16.14.255, dentro da sub-rede definida pela m\u00e1scara 255.255.240.0. Essa configura\u00e7\u00e3o permite que os endere\u00e7os atribu\u00eddos pelo DHCP fiquem restritos a uma parte do espa\u00e7o de endere\u00e7amento dispon\u00edvel, ajudando a controlar e organizar os acessos dentro da rede.  </li> <li> <p>Desabilitamos o DHCP no roteador, para que o MaaS seja o respons\u00e1vel pela distribui\u00e7\u00e3o dos IPs.</p> </li> <li> <p>Verifica\u00e7\u00f5es de Conectividade:   Validamos a configura\u00e7\u00e3o de rede realizando pings para <code>8.8.8.8</code> e <code>www.google.com</code>, assegurando que o roteamento dos pacotes e a resolu\u00e7\u00e3o de URLs estejam funcionando corretamente.</p> </li> </ul>"},{"location":"roteiro1/main/#o-que-e-o-ssh-e-como-ele-foi-configurado","title":"O que \u00e9 o SSH e como ele foi Configurado","text":"<p>SSH (Secure Shell) \u00e9 um protocolo que permite o acesso remoto seguro a sistemas, criando um canal criptografado entre o cliente e o servidor. Isso garante que as informa\u00e7\u00f5es transmitidas, como comandos e credenciais, estejam protegidas contra intercepta\u00e7\u00f5es.</p> <ol> <li> <p>Gera\u00e7\u00e3o do Par de Chaves:    Utilizamos <code>ssh-keygen -t -rsa</code> para criar um par de chaves (p\u00fablica e privada), permitindo a autentica\u00e7\u00e3o sem a necessidade de enviar senhas em texto claro.</p> </li> <li> <p>Distribui\u00e7\u00e3o da Chave P\u00fablica:    A chave p\u00fablica gerada foi copiada para o servidor (NUC main), possibilitando que o servidor autentique o cliente que possui a chave privada correspondente. Vale ressaltar que esse servi\u00e7o do ssh trabalha na porta 22.</p> </li> </ol>"},{"location":"roteiro1/main/#3-comissionamento-dos-servidores-e-criacao-de-ovs-bridge","title":"3. Comissionamento dos Servidores e Cria\u00e7\u00e3o de OVS Bridge","text":"<ul> <li>Comissionamento dos Servidores:   No Dashboard do MaaS, cadastramos os hosts (server1 at\u00e9 server5) e configuramos a op\u00e7\u00e3o de Power Type para Intel AMT. Foram inseridos os seguintes detalhes:  </li> <li>MAC Address (anotado previamente).  </li> <li>Senha padr\u00e3o: <code>CloudComp6s!</code>.  </li> <li>IP do AMT, no formato <code>172.16.15.X</code> (onde X corresponde ao id do servidor, por exemplo, server1 = 172.16.15.1).   Ap\u00f3s o boot via PXE, os servidores devem aparecer com o status \"Ready\", indicando que os par\u00e2metros de hardware (CPU, mem\u00f3ria, SSD e rede) foram detectados corretamente.   Tamb\u00e9m adicionamos o roteador como device no Dashboard do MaaS.</li> </ul> <p>Obs: O server 1 da nossa cloud estava enfrentando problemas para ser encontrado. Como esse roteiro n\u00e3o necessitava a utiliza\u00e7\u00e3o de todos os servers, seguimos utilizando os demais.</p> <ul> <li>Cria\u00e7\u00e3o de OVS Bridge:   Para reduzir a necessidade de duas interfaces de rede f\u00edsicas, configuramos uma Open vSwitch (OVS) bridge.  </li> <li>A ponte \u00e9 criada a partir da interface padr\u00e3o <code>enp1s0</code> e nomeada br-ex.  </li> <li>Essa configura\u00e7\u00e3o \u00e9 aplicada em todos os cinco n\u00f3s, garantindo flexibilidade e suporte ao OVN Chassis.</li> </ul>"},{"location":"roteiro1/main/#4-configuracao-de-nat-e-acesso-remoto","title":"4. Configura\u00e7\u00e3o de NAT e Acesso Remoto","text":""},{"location":"roteiro1/main/#o-que-e-nat","title":"O que \u00e9 NAT?","text":"<p>O NAT (Network Address Translation) \u00e9 uma t\u00e9cnica que permite que dispositivos de uma rede privada compartilhem um \u00fanico endere\u00e7o IP p\u00fablico para acessar a internet. Ele realiza a tradu\u00e7\u00e3o dos endere\u00e7os IP privados para um endere\u00e7o p\u00fablico e vice-versa, garantindo que os pacotes de dados sejam direcionados corretamente entre a rede interna e a externa.</p>"},{"location":"roteiro1/main/#como-o-nat-funciona-na-nossa-configuracao","title":"Como o NAT Funciona na Nossa Configura\u00e7\u00e3o","text":"<ul> <li> <p>Tradu\u00e7\u00e3o de Endere\u00e7os:   Cada dispositivo na rede local possui um endere\u00e7o IP privado (por exemplo, na faixa 172.16.0.0/20). Quando um dispositivo envia dados para a internet, o roteador que realiza o NAT substitui o endere\u00e7o IP privado pelo endere\u00e7o IP p\u00fablico configurado. Dessa forma, o tr\u00e1fego de sa\u00edda parece originar de um \u00fanico IP p\u00fablico.</p> </li> <li> <p>Port Forwarding para Acesso Remoto:   Para permitir a conex\u00e3o remota ao servidor principal (main) pela porta 22, configuramos um redirecionamento de porta (port forwarding). Isso faz com que qualquer conex\u00e3o que chegue \u00e0 porta 22 do endere\u00e7o IP p\u00fablico seja encaminhada para o IP fixo do servidor main (172.16.0.3).</p> </li> </ul>"},{"location":"roteiro1/main/#configuracao-do-nat-e-das-regras-de-gerenciamento-no-roteador","title":"Configura\u00e7\u00e3o do NAT e das Regras de Gerenciamento no Roteador","text":"<ul> <li> <p>Acesso \u00e0 Interface do Roteador:   A configura\u00e7\u00e3o \u00e9 realizada atrav\u00e9s da interface administrativa do roteador, acess\u00edvel via navegador atrav\u00e9s do IP do dispositivo.</p> </li> <li> <p>Defini\u00e7\u00e3o das Regras de NAT:   Na interface do roteador, configuramos as seguintes regras:</p> <ol> <li>Regra de NAT: Define que todos os dispositivos da subrede (172.16.0.0/20) usem o endere\u00e7o IP p\u00fablico do roteador para acessar a internet.</li> <li> <p>Port Forwarding: Cria uma regra para redirecionar conex\u00f5es que chegam na porta 22 para o IP 172.16.0.3, permitindo o acesso remoto seguro ao servidor main.</p> </li> <li> <p>Regras de Gerenciamento Remoto: Adicionalmente, foi criada uma regra que permite o acesso remoto ao pr\u00f3prio roteador (regra de gest\u00e3o), configurada para aceitar conex\u00f5es de qualquer endere\u00e7o (0.0.0.0/0). Essa regra possibilita a administra\u00e7\u00e3o do roteador remotamente.</p> </li> </ol> </li> </ul> <p>Essa configura\u00e7\u00e3o garante que a rede local tenha acesso \u00e0 internet por meio de um \u00fanico endere\u00e7o IP p\u00fablico, ao mesmo tempo que permite o gerenciamento remoto seguro tanto do servidor main quanto do roteador.</p>"},{"location":"roteiro1/main/#bare-metal-aplicacao","title":"Bare Metal - Aplica\u00e7\u00e3o","text":"<p>Nesta etapa, realizamos o deploy manual de uma aplica\u00e7\u00e3o simples em Django utilizando a infraestrutura configurada anteriormente na nuvem MaaS. \u00c9 importante notar que, devido a problemas t\u00e9cnicos, o server originalmente designado como \"server1\" n\u00e3o funcionou corretamente. Dessa forma, o servidor que inicialmente seria o server1 passou a ser considerado como server2, o que acarretou um reajuste na numera\u00e7\u00e3o dos demais servidores (server2 passou a ser server3, e assim por diante).</p> <p>Al\u00e9m disso, durante a instala\u00e7\u00e3o das imagens nos servidores, encontramos um problema inesperado: a Canonical atualizou a imagem do Ubuntu, alterando algum componente essencial e rompendo a compatibilidade. Para resolver esse problema, foi necess\u00e1rio atualizar o firmware em todas as m\u00e1quinas para que o Ubuntu 22.04 funcionasse corretamente.</p>"},{"location":"roteiro1/main/#1-ajuste-no-dns-do-servidor","title":"1. Ajuste no DNS do Servidor","text":"<p>Antes de iniciar o deploy, foi preciso ajustar a configura\u00e7\u00e3o do DNS:</p> <ul> <li>Acesse a aba Subnets no MaaS e edite a subnet <code>172.16.0.0/20</code>, alterando o campo Subnet summary para usar o DNS do Insper (<code>172.20.129.131</code>).</li> </ul>"},{"location":"roteiro1/main/#2-primeira-parte-configuracao-do-banco-de-dados","title":"2. Primeira Parte: Configura\u00e7\u00e3o do Banco de Dados","text":"<p>Utilizamos o PostgreSQL, um servidor de banco de dados robusto e amplamente utilizado em projetos open source, conforme os passos abaixo:</p> <ul> <li>Deploy do Ubuntu:   No MaaS, foi realizado o deploy do Ubuntu 22.04 no servidor designado (que originalmente seria o server1, mas considerando o ajuste, este \u00e9 agora o server2).</li> <li>Instala\u00e7\u00e3o e Configura\u00e7\u00e3o do PostgreSQL:   No terminal do servidor (acessado via SSH), executamos os seguintes procedimentos:</li> <li>Atualiza\u00e7\u00e3o do sistema.</li> </ul> sudo apt update <ul> <li>Instala\u00e7\u00e3o dos pacotes do PostgreSQL e suas contribui\u00e7\u00f5es.</li> </ul> sudo apt install postgresql postgresql-contrib -y <ul> <li>Cria\u00e7\u00e3o de um usu\u00e1rio para a aplica\u00e7\u00e3o (usu\u00e1rio <code>cloud</code> com senha <code>cloud</code>).</li> </ul> sudo su - postgrescreateuser -s cloud -W <ul> <li>Cria\u00e7\u00e3o de uma base de dados (por exemplo, <code>tasks</code>).</li> </ul> createdb -O cloud tasks <ul> <li>Edi\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o do PostgreSQL para que o servi\u00e7o aceite conex\u00f5es remotas (definindo <code>listen_addresses = '*'</code>).</li> </ul> nano /etc/postgresql/14/main/postgresql.conf <ul> <li>Ajuste no arquivo de controle de acesso (<code>pg_hba.conf</code>) para liberar conex\u00f5es vindas da subnet <code>172.16.0.0/20</code>.</li> <li>Libera\u00e7\u00e3o da porta (5432) no firewall e reinicializa\u00e7\u00e3o do servi\u00e7o.</li> </ul> sudo ufw allow 5432/tcpsudo systemctl restart postgresql"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Verificando se o banco est\u00e1 funcionando</p> <p></p> <p>Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional</p> <p></p> <p>Verifica se iniciou sem erro </p> <p></p> <p>Acessivel na pr\u00f3pria maquina na qual ele foi implantado</p> <p></p> <p>Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN na porta 5432.</p>"},{"location":"roteiro1/main/#3-parte-ii-deploy-da-aplicacao-django","title":"3. Parte II: Deploy da Aplica\u00e7\u00e3o Django","text":"<ul> <li>Reserva e Deploy da M\u00e1quina:   Utilizando o MaaS CLI, reservamos uma m\u00e1quina (originalmente designada como server2, mas, com o ajuste, ela se torna o novo server3 conforme a sequ\u00eancia), e realizamos o deploy atrav\u00e9s do CLI do MaaS. Vale ressaltar que, por conta do problema com a imagem, tivemos que refazer todos os deploys manualmente pelo MaaS.</li> <li>Clone e Instala\u00e7\u00e3o da Aplica\u00e7\u00e3o:   Acessamos o servidor via SSH, clonamos o reposit\u00f3rio da aplica\u00e7\u00e3o Django e executamos o script de instala\u00e7\u00e3o (<code>install.sh</code>).</li> </ul> maas [login] machine deploy [system_id]git clone https://github.com/raulikeda/tasks.git./install.shsudo reboot <ul> <li> <p>Instalando e configurando o Django (Parte da Tarefa 3):</p> <p>Ap\u00f3s o deploy manual da nossa m\u00e1quina, por conta do erro, para baixar a aplica\u00e7\u00e3o do Django, entramos no server3 e rodamos os seguintes comandos para instalar o Django:</p> <p>sudo apt install python3-psycopg2pip install django --break-system-packages </p> <p>Al\u00e9m disso, para a aplica\u00e7\u00e3o funcionar, modificamos o arquivo <code>settings.py</code> do Django para indicar que o banco de dados que iremos utilizar estar\u00e1 no server2 e ajustamos os <code>ALLOWED_HOSTS</code> para permitir o acesso sem problemas a partir do main.</p> <p>nano portfolio/settings.py </p> </li> <li> <p>Fazendo com que o server3 Enxergue o server2</p> <p>Como apenas o servidor main consegue visualizar todas as m\u00e1quinas (devido \u00e0 sua fun\u00e7\u00e3o de controlador do MaaS), precisamos informar o server3 sobre a localiza\u00e7\u00e3o do server2, que hospeda o PostgreSQL para a aplica\u00e7\u00e3o Django. Para isso, editamos o arquivo <code>/etc/hosts</code> no server3 e adicionamos uma entrada que associa o nome (\"server2\") ao respectivo endere\u00e7o IP.</p> <p>Essa a\u00e7\u00e3o garante que, mesmo sem acesso direto \u00e0 tabela de DNS da rede, o server3 consiga resolver o nome do server2 e estabelecer comunica\u00e7\u00e3o com ele. Al\u00e9m disso, desabilitamos a configura\u00e7\u00e3o para que essa altera\u00e7\u00e3o n\u00e3o seja perdida ap\u00f3s uma reinicializa\u00e7\u00e3o.</p> <p>sudo nano /etc/hosts </p> </li> </ul> <p>Teste da Aplica\u00e7\u00e3o:   Ap\u00f3s reiniciar a m\u00e1quina, testamos o acesso \u00e0 aplica\u00e7\u00e3o via terminal do MaaS utilizando um comando para verificar a porta 8080.</p> wget http://172.16.0.15:8080/admin/ <p>Para acesso no browser, criamos um t\u00fanel SSH redirecionando a porta 8080 do servidor para a porta 8001 local.</p> ssh cloud@10.103.1.19 -L 8001:172.16.0.15:8080 <p>Esse t\u00fanel faz com que utilizemos a porta 8001 do computador local para acessar o que est\u00e1 na porta 8080 do server3. Isso significa que qualquer solicita\u00e7\u00e3o feita para http://localhost:8001/admin/ ser\u00e1 redirecionada para http://172.16.0.15:8080/admin/. Dessa forma, podemos acessar a interface de administra\u00e7\u00e3o do django do server3 localmente atrav\u00e9s do navegador web.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<p>Dashboard do MAAS com Server 2 e 3 deployados</p> <p></p> <p>Comprova\u00e7\u00e3o das imagens do Ubuntu sincronizadas</p> <p> </p> <p>Todas as m\u00e1quinas passaram nos testes de hardware e commissioning com status \"OK\"</p>"},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<p>Dashboard do MAAS com os IPS das m\u00e1quinas</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server3</p> <p></p> <p>Comprovando que estamos rodando no server3</p>"},{"location":"roteiro1/main/#4-deploy-automatizado-com-ansible","title":"4. Deploy Automatizado com Ansible","text":"<p>O Ansible \u00e9 uma ferramenta de automa\u00e7\u00e3o que permite gerenciar configura\u00e7\u00f5es e implanta\u00e7\u00f5es de forma padronizada e repet\u00edvel. Diferente do processo manual, ele garante que os passos de instala\u00e7\u00e3o sejam executados da mesma forma em todas as m\u00e1quinas, sem interferir em estados intermedi\u00e1rios \u2013 um conceito conhecido como idempot\u00eancia.</p>"},{"location":"roteiro1/main/#o-que-acontece-durante-o-deploy-automatizado","title":"O Que Acontece Durante o Deploy Automatizado?","text":"<ol> <li> <p>Provisionamento da M\u00e1quina de Destino:    Ap\u00f3s solicitar o deploy via MaaS, uma nova m\u00e1quina (agora designada como server4) \u00e9 alocada para a aplica\u00e7\u00e3o.</p> </li> <li> <p>Instala\u00e7\u00e3o do Ansible no Servidor Principal:    Instalamos o Ansible no servidor principal (main). Isso transforma o main em um controlador central que se conecta \u00e0s m\u00e1quinas remotas para executar os comandos necess\u00e1rios.</p> </li> </ol> sudo apt install ansible <ol> <li>Baixar o Playbook:    Um playbook \u00e9 um arquivo YAML que cont\u00e9m uma s\u00e9rie de instru\u00e7\u00f5es que definem como a aplica\u00e7\u00e3o Django deve ser instalada e configurada. Ao baixar esse arquivo, garantimos que todos os passos necess\u00e1rios ser\u00e3o executados de forma padronizada.</li> </ol> wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml <ol> <li>Execu\u00e7\u00e3o do Playbook:    Com o playbook em m\u00e3os, utilizamos o comando do Ansible para execut\u00e1-lo, passando o IP do server4 como vari\u00e1vel extra. Esse processo automatiza a instala\u00e7\u00e3o da aplica\u00e7\u00e3o Django, realizando as mesmas a\u00e7\u00f5es que seriam feitas manualmente (como instala\u00e7\u00e3o de pacotes, configura\u00e7\u00e3o de servi\u00e7os e deploy da aplica\u00e7\u00e3o), mas sem a necessidade de interven\u00e7\u00e3o manual em cada servidor.</li> </ol> ansible-playbook tasks-install-playbook.yaml --extra-vars server=172.16.0.17 <ol> <li> <p>Fazendo com que o server4 enxergue o server2</p> <p>Seguindo o mesmo processo feito para o server3, devemos fazer para o server4 enxergar o server2. Para isso entramos nas configura\u00e7\u00d5es do <code>/etc/hosts</code> no server3 e adicionamos uma entrada que associa o nome (\"server2\") ao respectivo endere\u00e7o IP.</p> </li> </ol>"},{"location":"roteiro1/main/#por-que-usar-o-ansible","title":"Por Que Usar o Ansible?","text":"<ul> <li> <p>Idempot\u00eancia:   O Ansible garante que, mesmo se executarmos o playbook m\u00faltiplas vezes, o sistema sempre alcan\u00e7ar\u00e1 o estado desejado, sem efeitos colaterais indesejados.</p> </li> <li> <p>Gerenciamento Simult\u00e2neo de V\u00e1rias M\u00e1quinas:   Permite que um \u00fanico playbook seja aplicado a m\u00faltiplos servidores de uma vez, garantindo uniformidade na instala\u00e7\u00e3o e configura\u00e7\u00e3o da aplica\u00e7\u00e3o.</p> </li> <li> <p>Facilidade de Automa\u00e7\u00e3o:   Com um arquivo de configura\u00e7\u00e3o centralizado, qualquer altera\u00e7\u00e3o ou corre\u00e7\u00e3o pode ser aplicada de maneira r\u00e1pida e consistente em todos os n\u00f3s.</p> </li> <li> <p>Redu\u00e7\u00e3o de Erros Humanos:   Automatizando o processo, minimizamos a chance de erros que podem ocorrer durante a configura\u00e7\u00e3o manual de cada servidor.</p> </li> </ul>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<p>Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server3</p> <p></p> <p>Comprovando que estamos rodando no server3</p> <p></p> <p>Aplica\u00e7\u00e3o django rodando no server4</p> <p></p> <p>Comprovando que estamos rodando no server4</p> <p>A diferen\u00e7a entre instalar manualmente a aplica\u00e7\u00e3o Django e utilizar o Ansible \u00e9 que, na instala\u00e7\u00e3o manual, cada comando deve ser executado individualmente em cada servidor, o que aumenta o risco de erros e pode resultar em configura\u00e7\u00f5es inconsistentes. J\u00e1 com o Ansible, um playbook automatiza todo o processo, garantindo que os mesmos passos sejam aplicados de forma id\u00eantica em todos os n\u00f3s, tornando o deploy mais r\u00e1pido, confi\u00e1vel e escal\u00e1vel, al\u00e9m de reduzir significativamente a possibilidade de erros humanos.</p>"},{"location":"roteiro1/main/#5-balancamento-de-carga-com-proxy-reverso","title":"5. Balancamento de Carga com Proxy Reverso","text":"<p>O balanceamento de carga com proxy reverso tem como objetivo centralizar o acesso \u00e0 aplica\u00e7\u00e3o, de forma que uma \u00fanica entrada redirecione as requisi\u00e7\u00f5es para v\u00e1rios servidores que hospedam a aplica\u00e7\u00e3o Django. Essa abordagem \u00e9 essencial para garantir alta disponibilidade e redund\u00e2ncia: se um dos servidores falhar, os outros continuam atendendo as requisi\u00e7\u00f5es, mantendo a estabilidade do servi\u00e7o.</p> <p>Nesse cen\u00e1rio, instalamos o NGINX no server5 para atuar como proxy reverso. O primeiro passo \u00e9 instalar o NGINX, utilizando o gerenciador de pacotes do sistema:</p> sudo apt-get install nginx <p>Ap\u00f3s a instala\u00e7\u00e3o, editamos o arquivo de configura\u00e7\u00e3o do NGINX, localizado em <code>/etc/nginx/sites-available/default</code>. Nele, definimos o bloco upstream que aponta para os servidores backend onde a aplica\u00e7\u00e3o Django est\u00e1 rodando:</p> <pre><code>upstream backend { server 172.16.0.15:8080; server 172.16.0.17:8080; }\n</code></pre> <p>Em seguida, configuramos o servidor virtual para encaminhar todas as requisi\u00e7\u00f5es que chegarem na porta 80 para o grupo de servidores definido acima, utilizando a diretiva <code>proxy_pass</code>:</p> <pre><code>server { location / { proxy_pass http://backend; } }\n</code></pre> <p>Al\u00e9m disso comentamos algumas linhas para que n\u00e3o houvesse conflitos das informa\u00e7\u00d5es.</p> <p>Ap\u00f3s todas as configura\u00e7\u00f5es realizadas no nginx, reiniciamos o servi\u00e7o</p> sudo service nginx restart <p>A partir deste momento, o NGINX distribuir\u00e1 as requisi\u00e7\u00f5es recebidas entre os servidores definidos no bloco upstream, utilizando o algoritmo Round Robin por padr\u00e3o. Isso melhora a escalabilidade e a toler\u00e2ncia a falhas, pois se um dos servidores ficar indispon\u00edvel, o outro continuar\u00e1 atendendo \u00e0s requisi\u00e7\u00f5es.</p> <p>Para verificar o funcionamento, modificamos a fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> em cada inst\u00e2ncia da aplica\u00e7\u00e3o Django, atribuindo mensagens distintas. Assim, ao acessar a aplica\u00e7\u00e3o por meio do server5, podemos observar que as respostas alternam entre as mensagens, confirmando que o balanceamento de carga est\u00e1 ativo e direcionando as requisi\u00e7\u00f5es para cada servidor de forma rotativa.</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<p>Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs</p> <p></p> <p>Mudan\u00e7a da mensagem no views.py server3</p> <p></p> <p>Mudan\u00e7a da mensagem no views.py server4</p> <p></p> <p>Requisi\u00e7\u00e3o com print da mensagem do server3</p> <p></p> <p>Requisi\u00e7\u00e3o com print da mensagem do server4</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Durante a configura\u00e7\u00e3o e implementa\u00e7\u00e3o da nuvem bare-metal, surgiram diversos pontos de discuss\u00e3o e aprendizado:</p> <ul> <li> <p>Configura\u00e7\u00e3o de Rede e MaaS: Embora o MaaS simplifique a orquestra\u00e7\u00e3o de hardware, entender sua l\u00f3gica de DHCP e a intera\u00e7\u00e3o com o roteador demandou um tempo consider\u00e1vel. O fato de o switch n\u00e3o possuir DHCP embutido exigiu aten\u00e7\u00e3o redobrada na defini\u00e7\u00e3o dos endere\u00e7os IP e no desvio de fun\u00e7\u00f5es do roteador para o MaaS.  </p> </li> <li> <p>Problemas T\u00e9cnicos e Solu\u00e7\u00f5es: A indisponibilidade do servidor que originalmente seria o server1 mostrou a import\u00e2ncia de ter planos de conting\u00eancia. O ajuste na numera\u00e7\u00e3o dos servidores e a atualiza\u00e7\u00e3o de firmware para corrigir a compatibilidade com o Ubuntu 22.04 demonstram como ambientes de produ\u00e7\u00e3o podem exigir adapta\u00e7\u00f5es e reconfigura\u00e7\u00f5es inesperadas.</p> </li> <li> <p>Instala\u00e7\u00e3o Manual vs. Ansible: A instala\u00e7\u00e3o manual do Django e do banco de dados PostgreSQL, embora did\u00e1tica para fins de aprendizado, revelou-se propensa a erros e repeti\u00e7\u00f5es. Em contrapartida, o uso do Ansible permitiu automatizar a configura\u00e7\u00e3o de m\u00faltiplos servidores de forma consistente, evidenciando a escalabilidade e a confiabilidade proporcionadas pelas ferramentas de automa\u00e7\u00e3o.</p> </li> <li> <p>Balanceamento de Carga: Configurar o NGINX como proxy reverso enfatizou a import\u00e2ncia de uma \u00fanica entrada de acesso para distribuir requisi\u00e7\u00f5es entre v\u00e1rios servidores. Esse passo \u00e9 fundamental para garantir alta disponibilidade, especialmente em cen\u00e1rios de produ\u00e7\u00e3o que exigem redund\u00e2ncia e escalabilidade.</p> </li> <li> <p>Dificuldades e Facilidade: A maior dificuldade relatada foi lidar com ajustes de firmware e vers\u00f5es de imagem que mudaram subitamente. J\u00e1 a maior facilidade percebida foi a automa\u00e7\u00e3o do deploy com Ansible, que simplificou a replica\u00e7\u00e3o das configura\u00e7\u00f5es e reduziu erros humanos.</p> </li> </ul>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro proporcionou uma vis\u00e3o completa do processo de cria\u00e7\u00e3o e gest\u00e3o de uma nuvem bare-metal. Iniciando pela configura\u00e7\u00e3o da infraestrutura de rede e passando pela instala\u00e7\u00e3o manual de servi\u00e7os cr\u00edticos como o PostgreSQL e o Django, o aprendizado foi consolidado ao automatizar tarefas com Ansible e ao implementar um balanceador de carga via NGINX.</p> <p>No final, foi poss\u00edvel concluir que:</p> <ol> <li> <p>Gerenciamento Centralizado \u00e9 Essencial: O MaaS demonstrou ser uma ferramenta valiosa para orquestrar o provisionamento e o controle de m\u00faltiplos servidores, otimizando a aloca\u00e7\u00e3o de recursos e o gerenciamento de IPs.</p> </li> <li> <p>Automa\u00e7\u00e3o Reduz Erros e Garante Consist\u00eancia: A diferen\u00e7a entre instalar servi\u00e7os manualmente e utilizar o Ansible evidenciou o quanto a automa\u00e7\u00e3o traz confiabilidade, escalabilidade e facilidade de manuten\u00e7\u00e3o.</p> </li> <li> <p>Alta Disponibilidade e Redund\u00e2ncia: A configura\u00e7\u00e3o de um proxy reverso (NGINX) para balanceamento de carga exemplificou a import\u00e2ncia de garantir que a aplica\u00e7\u00e3o se mantenha dispon\u00edvel mesmo diante de falhas pontuais em um ou mais servidores.</p> </li> </ol> <p>Em suma, este roteiro possibilitou a experi\u00eancia pr\u00e1tica de construir uma infraestrutura de cloud bare-metal, destacando desde os fundamentos de rede at\u00e9 a entrega de aplica\u00e7\u00f5es de forma robusta e escal\u00e1vel, aproximando o aprendizado dos desafios reais encontrados em data centers e ambientes de produ\u00e7\u00e3o.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#objetivo","title":"Objetivo","text":"<p>Este roteiro tem como objetivo apresentar os fundamentos do Juju como uma plataforma de orquestra\u00e7\u00e3o de deployment para aplica\u00e7\u00f5es distribu\u00eddas. </p> <p>O foco est\u00e1 em compreender como o Juju simplifica a configura\u00e7\u00e3o e o gerenciamento de servi\u00e7os. Al\u00e9m disso, ser\u00e3o abordados os conceitos de comunica\u00e7\u00e3o entre aplica\u00e7\u00f5es e servi\u00e7os, destacando como o Juju automatiza a intera\u00e7\u00e3o entre os componentes de um sistema distribu\u00eddo.</p> <p>Para produ\u00e7\u00e3o desse roteiro foi necess\u00e1ria a conclus\u00e3o da etapa do Bare Metal (Roteiro 1) garantindo que a infraestrutura de base esteja configurada corretamente antes de seguir com a implementa\u00e7\u00e3o do Juju.</p>"},{"location":"roteiro2/main/#material-utilizado","title":"Material Utilizado","text":"<p>1 NUC (main) com 10Gb e 1 SSD (120Gb)</p> <p>1 NUC (server1) com 12Gb e 1 SSD (120Gb)</p> <p>1 NUC (server2) com 16Gb e 2 SSD (120Gb+120Gb)</p> <p>3 NUCs (server3, server4 e server5) com 32Gb e 2 SSD (120Gb+120Gb)</p> <p>1 Switch DLink DSG-1210-28 de 28 portas</p> <p>1 Roteador TP-Link TL-R470T+   </p>"},{"location":"roteiro2/main/#criando-a-infraestrutura","title":"Criando a Infraestrutura","text":"<p>Nesta etapa, configuraremos o Juju para gerenciar a infraestrutura de servidores anteriormente criadas e configuradas e que est\u00e3o sendo gerenciados pelo MAAS.</p> <p>Diferente do Ansible, que utilizamos para automatizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o de servi\u00e7os no roteiro anterior, o Juju consegue integrar-se diretamente ao MAAS, atuando como um orquestrador de deploy que provisiona e gerencia os recursos computacionais dinamicamente.</p> <p>Fizemos o release do Postgres e Django do roteiro anterior, e iniciamos pela instala\u00e7\u00e3o do Juju.</p>"},{"location":"roteiro2/main/#1-instalacao-e-configuracao-do-juju","title":"1. Instala\u00e7\u00e3o e configura\u00e7\u00e3o do Juju","text":"<p>Utilizamos o Juju 3.6 para este roteiro, que ser\u00e1 utilizado para gerenciar a infraestrutura de servidores.</p> <pre><code>sudo snap install juju --channel 3.6\n</code></pre> <p>O Juju utilizar\u00e1 o MAAS para provedor de m\u00e1quinas e sistema operacional, ou seja, haver\u00e1 uma integra\u00e7\u00e3o entre o Juju e o MAAS. Para isso, foi necess\u00e1rio configur\u00e1-lo para que possa enxergar o MAAS. Foi feita a cria\u00e7\u00e3o de dois arquivos para defini\u00e7\u00e3o do cloud e do credential.</p> <p>Primeiro criamos o arquivo <code>maas-cloud.yaml</code> que informa ao Juju onde encontrar o servi\u00e7o MAAS e como ele deve se autenticar. Utilizamos o endere\u00e7o local e sua porta padr\u00e3o para conex\u00e3o que \u00e9 5240.</p> <pre><code>clouds:\n    maas-one:\n        type: maas\n        auth-types: [oauth1]\n        endpoint: http://172.16.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, importamos o arquivo de configura\u00e7\u00e3o para o Juju.</p> <pre><code>juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>Depois, criamos o arquivo <code>maas-creds.yaml</code> que cont\u00e9m as credenciais de acesso ao MAAS. Necess\u00e1rio para que o Juju gerencie as m\u00e1quinas dentro da cloud definida</p> <p><pre><code>credentials:\n    maas-one:\n    anyuser:\n        auth-type: oauth1\n        maas-oauth: &lt;API KEY&gt;\n</code></pre> OBS: API KEY \u00e9 a chave que permite a autentica\u00e7\u00e3o, gerada dentro do MAAS, no menu do usu\u00e1rio</p> <p>Em seguida, importamos o arquivo de credenciais para o Juju.</p> <pre><code>juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#2-criando-o-controller-do-juju","title":"2. Criando o controller do Juju","text":"<p>O Juju precisa de um controller, uma m\u00e1quina exclusiva para gerenciar a orquestra\u00e7\u00e3o dos servi\u00e7os. Al\u00e9m disso, ele \u00e9 o componente central que gerencia o ciclo de vida das aplica\u00e7\u00f5es. Ele \u00e9 respons\u00e1vel por alocar m\u00e1quinas, monitorar os servi\u00e7os e gerenciar o deploy de novas aplica\u00e7\u00f5es.</p> <p>Para isso, utilizaremos o server1, que foi marcado com a tag juju dentro do painel do MAAS.</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>O par\u00e2metro <code>--bootstrap-series=jammy</code> define a vers\u00e3o do sistema operacional que ser\u00e1 utilizada (Ubuntu 22.04). O par\u00e2metro <code>--constraints tags=juju</code> define que o controller ser\u00e1 criado na m\u00e1quina que possui a tag juju.</p> <p>O comando acima demora um pouco para ser executado, porque diversas etapas s\u00e3o realizadas para configurar o controller, como:</p> <ul> <li> <p>Provisionamento da m\u00e1quina no MAAS</p> </li> <li> <p>Instala\u00e7\u00e3o do Juju</p> </li> <li> <p>Configura\u00e7\u00e3o do Juju para se conectar ao MAAS</p> </li> <li> <p>Testes de conectividade, e verifica\u00e7\u00f5es finais</p> </li> </ul> <p>Por fim criamos o modelo 'openstack', que \u00e9 o ambiente onde as aplica\u00e7\u00f5es ser\u00e3o instaladas.</p> <p>Os modelos s\u00e3o ambientes isolados dentro de um controller, usados para organizar diferentes aplica\u00e7\u00f5es</p> <pre><code>juju add-model --config default-series=jammy openstack\n</code></pre>"},{"location":"roteiro2/main/#utilizando-a-infraestrutura","title":"Utilizando a Infraestrutura","text":"<p>Com o controller do Juju configurado, podemos come\u00e7ar a utilizar a infraestrutura de servidores para o deployment de aplica\u00e7\u00f5es. </p> <p>Nesta etapa, trabalharemos com duas aplica\u00e7\u00f5es essenciais para monitoramento: Grafana e Prometheus. Sendo o Grafana respons\u00e1vel por simplificar a apresenta\u00e7\u00e3o visual de dados, e o Prometheus por coletar e armazenar essas m\u00e9tricas, funcionando como um banco de dados. </p> <p>Com ambas instaladas, ser\u00e1 poss\u00edvel acompanhar o desempenho e a integridade dos servi\u00e7os em execu\u00e7\u00e3o atrav\u00e9s de um dashboard.</p>"},{"location":"roteiro2/main/#1-deploy-do-dashboard-do-juju","title":"1. Deploy do Dashboard do Juju","text":"<p>O primeiro passo foi instalar o dashboard do Juju, para isso, voltamos para o controller do Juju para instalar o dashboard.</p> <pre><code>juju switch controller\n</code></pre> <p>Al\u00e9m disso, criamos a tag dashboard-juju no MAAS para que o Juju pudesse instalar o dashboard na m\u00e1quina correta. Essa tag foi atribu\u00edda ao server3 para o deploy do dashboard e utilizada com o par\u00e2metro <code>--constraints tags=dashboard-juju</code>.</p> <pre><code>juju deploy juju-dashboard dashboard --constraints tags=dashboard-juju\n</code></pre> <p>Ap\u00f3s o deploy, conseguimos integrar o controller com o dashboard do Juju e expor o servi\u00e7o para acesso externo.</p> <pre><code>juju integrate dashboard controller\n</code></pre> <pre><code>juju expose dashboard\n</code></pre> <p>Agora, nosso servi\u00e7o de dashboard do Juju est\u00e1 dispon\u00edvel para acesso externo. Com esse c\u00f3digo, obtivemos o endere\u00e7o e as credenciais para acesso.</p> <pre><code>juju dashboard\n</code></pre> <p>Al\u00e9m disso, foi necess\u00e1rio criar um t\u00fanel SSH para acessar o dashboard do Juju remotamente. O comando abaixo cria uma conex\u00e3o segura entre o nosso localhost e o server3, onde o dashboard do Juju est\u00e1 rodando.</p> <pre><code>ssh cloud@10.103.1.19 -L 31666:172.16.0.22:8080\n\n\n### 2. Deploy do Grafana e Prometheus\n\nCriamos uma pasta para armazenar os charms que ser\u00e3o utilizados no deploy das aplica\u00e7\u00f5es.\n\nOBS: **Charms** s\u00e3o pacotes de software que cont\u00eam tudo o que \u00e9 necess\u00e1rio para executar um servi\u00e7o. Eles funcionam como scripts encapsulados que definem como uma aplica\u00e7\u00e3o deve ser instalada, gerenciada e integrada com outras\n\n``` bash\nmkdir -p /home/cloud/charms\ncd /home/cloud/charms\n</code></pre> <p>Em seguida, baixamos os charms do Grafana e do Prometheus.</p> <pre><code>juju download grafana\n</code></pre> <pre><code>juju download prometheus2\n</code></pre> <p>O deploy das aplica\u00e7\u00f5es foi feito com auxilio do Juju, utilizando o modelo 'openstack' criado anteriormente. Isso foi importante para que n\u00e3o sobrecarreg\u00e1ssemos o controller com a instala\u00e7\u00e3o de aplica\u00e7\u00f5es em um \u00fanico modelo.</p> <p>Al\u00e9m disso, fizemos o deploy do Prometheus e do Grafana em m\u00e1quinas diferentes (server2 e server4, respectivamente), para garantir que os servi\u00e7os n\u00e3o competissem por recursos.</p> <pre><code>juju deploy ./prometheus2_XXX.charm\n</code></pre> <pre><code>juju deploy ./grafana_XXX.charm\n</code></pre> <p>OBS: XXX \u00e9 a vers\u00e3o do charm, que no caso foi r69</p> <p>Apesar disso, tivemos um problema com o deploy do Grafana, pois a vers\u00e3o do charm n\u00e3o enxergava a imagem do ubuntu 22.04, que era a vers\u00e3o utilizada nas m\u00e1quinas. Para resolver, o pr\u00f3prio Juju instalou a vers\u00e3o 20.04. No entanto, para concluir o processo, foi necess\u00e1rio utilizar a flag <code>--force</code> no comando de deploy para garantir que o processo fosse conclu\u00eddo.</p> <pre><code>juju deploy ./grafana_r69.charm --force\n</code></pre>"},{"location":"roteiro2/main/#3-integracao-entre-grafana-e-prometheus","title":"3. Integra\u00e7\u00e3o entre Grafana e Prometheus","text":"<p>No Juju, uma integra\u00e7\u00e3o \u00e9 uma conex\u00e3o entre duas ou mais aplica\u00e7\u00f5es. Essa conex\u00e3o \u00e9 estabelecida devido ao fato de uma aplica\u00e7\u00e3o ter um endpoint (ponto de comunica\u00e7\u00e3o) espec\u00edfico que permite a intera\u00e7\u00e3o com outras aplica\u00e7\u00f5es.</p> <p>Grafana pode se conectar ao Prometheus atrav\u00e9s de uma rela\u00e7\u00e3o, onde o Prometheus exp\u00f5e m\u00e9tricas e o Grafana as consome para exibir visualmente.</p> <p>Para isso, \u00e9 necess\u00e1rio configurar o Prometheus como um data source no Grafana.</p> <pre><code>juju integrate prometheus2:grafana\u2013source grafana:grafana-source \n</code></pre>"},{"location":"roteiro2/main/#4-acessando-o-grafana","title":"4. Acessando o Grafana","text":"<p>Para acessar o Grafana rodando no server4 por meio de um t\u00fanel SSH. Criamos uma conex\u00e3o segura entre o nosso localhost e o server4, utilizando a funcionalidade de redirecionamento de portas do SSH. </p> <p>Esse t\u00fanel permite que o tr\u00e1fego da porta 8001 no nosso computador local seja redirecionado para a porta 3000 no server4, onde o Grafana est\u00e1 ouvindo, como se estivesse rodando localmente em nossa m\u00e1quina.</p> <pre><code>ssh cloud@10.103.1.19 -L 8001:172.16.0.25:3000\n</code></pre> <p>Assim, ao acessar o endere\u00e7o <code>http://localhost:8001</code> no navegador, conseguimos visualizar o Grafana.</p> <p>OBS: Para conseguir acessar o Grafana, foi necess\u00e1rio pegar a senha gerada pelo Juju e inserir no login.</p> <pre><code>juju run grafana/1 get-admin-password\n</code></pre> <p>Por fim, criamos um novo dashboard no Grafana, onde adicionamos o Prometheus como data source.</p>"},{"location":"roteiro2/main/#tarefa-1","title":"Tarefa 1","text":"<p>Verificando se a integra\u00e7\u00e3o foi feita corretamente</p> <p></p> <p>Dashboard do MAAS com as m\u00e1quinas</p> <p></p> <p>Estado atual de todos os deployments no Juju</p> <p></p> <p>Tela do Dashboard do Grafana com o Prometheus como source</p> <p></p> <p>Acesso remoto ao Dashboard a partir da rede do Insper</p> <p></p> <p>Aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU</p>"},{"location":"roteiro2/main/#discussao","title":"Discuss\u00e3o","text":"<ol> <li> <p>A combina\u00e7\u00e3o entre Juju e MAAS simplificou a automa\u00e7\u00e3o do gerenciamento de infraestrutura, permitindo o provisionamento r\u00e1pido e eficiente de servidores f\u00edsicos e virtuais.</p> </li> <li> <p>O uso de charms para o deployment de servi\u00e7os garantiu uma instala\u00e7\u00e3o padronizada e a consist\u00eancia entre diferentes ambientes, o que facilita a replica\u00e7\u00e3o e a escalabilidade da infraestrutura.</p> </li> <li> <p>A utiliza\u00e7\u00e3o de tags no MAAS e modelos no Juju foi crucial para separar e organizar ambientes de forma eficiente, promovendo uma gest\u00e3o mais clara e estruturada da infraestrutura.</p> </li> <li> <p>A incompatibilidade de vers\u00e3o do charm Grafana com a vers\u00e3o do Ubuntu ataraplhou o desenvolvimento foi resolvida utilizando a flag --force, permitindo a continuidade do deployment sem maiores impactos.</p> </li> <li> <p>A integra\u00e7\u00e3o entre Grafana e Prometheus proporcionou uma vis\u00e3o clara e em tempo real do desempenho da infraestrutura, facilitando a an\u00e1lise e a resolu\u00e7\u00e3o de poss\u00edveis problemas.</p> </li> <li> <p>O SSH foi utilizado para acessar de forma segura os dashboards do Grafana, garantindo que a comunica\u00e7\u00e3o com a infraestrutura fosse protegida.</p> </li> </ol>"},{"location":"roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Em suma, a integra\u00e7\u00e3o entre Juju e MAAS, junto ao uso de charms para deployment, foi fundamental para automatizar e organizar de forma eficaz o gerenciamento da infraestrutura. A compatibilidade e os problemas t\u00e9cnicos foram superados com rapidez, garantindo uma implementa\u00e7\u00e3o bem-sucedida. O uso de ferramentas como Grafana e Prometheus permitiu monitorar em tempo real a performance, proporcionando visibilidade e controle essenciais para a otimiza\u00e7\u00e3o cont\u00ednua dos servi\u00e7os. A automa\u00e7\u00e3o oferecida pelo Juju se mostrou uma solu\u00e7\u00e3o poderosa, facilitando o gerenciamento e manuten\u00e7\u00e3o da infraestrutura.</p>"}]}